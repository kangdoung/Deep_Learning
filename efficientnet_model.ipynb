{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Efficientnet.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPBC7LaTf11vCGvCRNMlO1o",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "fec7f169d8bc4dc088568f72b724301b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_184a6f2010494d6d9040cf4227c90c7c",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_c4e0660969114c249a507e7a1d1001bc",
              "IPY_MODEL_d0df5b4cae9b4b26886a720087ef8310"
            ]
          }
        },
        "184a6f2010494d6d9040cf4227c90c7c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "c4e0660969114c249a507e7a1d1001bc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_8667cd3658b24e5d88d65d0908131fff",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 266860719,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 266860719,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_f296bae49777489dbfd1786b7ba98f10"
          }
        },
        "d0df5b4cae9b4b26886a720087ef8310": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_2e301a00b48541d3ac1041db17613b4a",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 254M/254M [00:05&lt;00:00, 52.2MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_5db16d3f5e654feaa7e749fa78af4759"
          }
        },
        "8667cd3658b24e5d88d65d0908131fff": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "f296bae49777489dbfd1786b7ba98f10": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "2e301a00b48541d3ac1041db17613b4a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "5db16d3f5e654feaa7e749fa78af4759": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/khangdltUIT/Deep_Learning/blob/main/efficientnet_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bX41yrtfhrmZ"
      },
      "source": [
        "# **Efficientnet: Rethinking model scaling for convolutional Neural Network**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LAr6BB37hwaw"
      },
      "source": [
        "## *Introduction*\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g-ILwU_ghyWK"
      },
      "source": [
        "Efficientnet was proposed at paper: *Efficientnet: Rethinking model scaling for convolutional Neural Network* by researchers at Google Brain. In that paper, reseachers proposed a compound scaling method which scale up not only depth dimension but also width and resoluton dimensions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X65T6-Ewi-jb"
      },
      "source": [
        "* Depth: simply means how deep the networks is which is equivalent to the number of layers in it\n",
        "* Width: simply means how wide the network is \n",
        "* Resolution: is simply the image resolution that is being passed to a CNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TidFcbi7iyiI"
      },
      "source": [
        "![model_scaling.png](data:image/png;base64,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)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gygViyYQjgKK"
      },
      "source": [
        "## **Implementation**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eixUgTnejmAc"
      },
      "source": [
        "In this section, I will use Pytorch frameword for implementaion Efficientnet.  \n",
        "Regarding the dataset, I use Dog/Cat Classification publshing on kaggle.com"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I6tXpMBnkNLj"
      },
      "source": [
        "### Step 01: Set up environment\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MskEYA3Bkxw8"
      },
      "source": [
        "##### Check GPU Info"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XKdedSgZhl_-",
        "outputId": "621905d9-9c22-47c7-ef9d-141021c62f89"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Thu Mar 25 01:35:42 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.56       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   35C    P8     7W /  75W |      0MiB /  7611MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MMpRGieSk2DV"
      },
      "source": [
        "#### Import Libraries used in this project"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tp0aAP5CkRG6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aef25c65-8528-4abe-89c3-c04f1e11654d"
      },
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as datasets\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset\n",
        "import os\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "import numpy as np\n",
        "from PIL import ImageFile\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from datetime import datetime\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import random \n",
        "from shutil import copyfile\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision.datasets import ImageFolder\n",
        "from PIL import Image\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import Dataset\n",
        "import numpy as np\n",
        "from sklearn.metrics import roc_auc_score\n",
        "import re\n",
        "import albumentations as albu\n",
        "from albumentations.pytorch import ToTensor\n",
        "import re\n",
        "import albumentations as albu\n",
        "from albumentations.pytorch import ToTensor\n",
        "\n",
        "print('Importing... done!')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Importing... done!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PvAwsJvVlKRB"
      },
      "source": [
        "### Step 02: CORONA CHEST DATASET"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BJqVHVR_oUxG"
      },
      "source": [
        "* Information:  \n",
        "The utility of this dataset has been confirmed by a senior radiologist in Tongji Hospital, Wuhan, China, who has performed diagnosis and treatment of a large number of COVID-19 patients during the outbreak of this disease between January and April."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HX4lFZb__nZT",
        "outputId": "8e49e08e-e453-42bc-add3-f154d640057a"
      },
      "source": [
        "!git clone https://github.com/UCSD-AI4H/COVID-CT"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'COVID-CT'...\n",
            "remote: Enumerating objects: 4, done.\u001b[K\n",
            "remote: Counting objects: 100% (4/4), done.\u001b[K\n",
            "remote: Compressing objects: 100% (4/4), done.\u001b[K\n",
            "remote: Total 5463 (delta 0), reused 0 (delta 0), pack-reused 5459\u001b[K\n",
            "Receiving objects: 100% (5463/5463), 1.09 GiB | 43.11 MiB/s, done.\n",
            "Resolving deltas: 100% (360/360), done.\n",
            "Checking out files: 100% (1048/1048), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oQZhq9XVAAdn"
      },
      "source": [
        "!unzip -q '/content/COVID-CT/Images-processed/CT_COVID.zip'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B-DdWu0KAKU-"
      },
      "source": [
        "!unzip -q '/content/COVID-CT/Images-processed/CT_NonCOVID.zip'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IoIBcbz6Bc8o"
      },
      "source": [
        "!mkdir Image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "83b8Pvs8BerK"
      },
      "source": [
        "!mv '/content/CT_COVID' '/content/Image'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KZOUySfvCrud"
      },
      "source": [
        "!mv '/content/CT_NonCOVID' '/content/Image'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MOOaf537AOmD"
      },
      "source": [
        "### Step 03: Define Data Loader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nfhQ66ZrAR2Q"
      },
      "source": [
        "def read_txt(txt_path):\n",
        "    with open(txt_path) as f:\n",
        "        lines = f.readlines()\n",
        "    txt_data = [line.strip() for line in lines]\n",
        "    return txt_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c9kwAbFJAYSE"
      },
      "source": [
        "class CovidCTDataset():\n",
        "    def __init__(self, root_dir, txt_COVID, txt_NonCOVID, transform=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            txt_path (string): Path to the txt file with annotations.\n",
        "            root_dir (string): Directory with all the images.\n",
        "            transform (callable, optional): Optional transform to be applied\n",
        "                on a sample.\n",
        "        File structure:\n",
        "        - root_dir\n",
        "            - CT_COVID\n",
        "                - img1.png\n",
        "                - img2.png\n",
        "                - ......\n",
        "            - CT_NonCOVID\n",
        "                - img1.png\n",
        "                - img2.png\n",
        "                - ......\n",
        "        \"\"\"\n",
        "        self.root_dir = root_dir\n",
        "        self.txt_path = [txt_COVID,txt_NonCOVID]\n",
        "        self.classes = ['CT_COVID', 'CT_NonCOVID']\n",
        "        self.num_cls = len(self.classes)\n",
        "        self.img_list = []\n",
        "        for c in range(self.num_cls):\n",
        "            cls_list = [[os.path.join(self.root_dir,self.classes[c],item), c] for item in read_txt(self.txt_path[c])]\n",
        "            self.img_list += cls_list\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.img_list)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if torch.is_tensor(idx):\n",
        "            idx = idx.tolist()\n",
        "\n",
        "        img_path = self.img_list[idx][0]\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        sample = {'img': image,\n",
        "                  'label': int(self.img_list[idx][1])}\n",
        "        return sample"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GZxX3wFDAjSs"
      },
      "source": [
        "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "train_transformer = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.RandomResizedCrop((224),scale=(0.5,1.0)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    normalize\n",
        "])\n",
        "\n",
        "val_transformer = transforms.Compose([\n",
        "    transforms.Resize(224),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    normalize\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7woXK5uxAn6M",
        "outputId": "94a04b39-deba-4e9e-b329-11829d69db1a"
      },
      "source": [
        "\n",
        "trainset = CovidCTDataset(root_dir='/content/Image',\n",
        "                            txt_COVID='/content/COVID-CT/Data-split/COVID/trainCT_COVID.txt',\n",
        "                            txt_NonCOVID='/content/COVID-CT/Data-split/NonCOVID/trainCT_NonCOVID.txt',\n",
        "                            transform= train_transformer)\n",
        "valset = CovidCTDataset(root_dir='/content/Image',\n",
        "                            txt_COVID='/content/COVID-CT/Data-split/COVID/valCT_COVID.txt',\n",
        "                            txt_NonCOVID='/content/COVID-CT/Data-split/NonCOVID/valCT_NonCOVID.txt',\n",
        "                            transform= val_transformer)\n",
        "testset = CovidCTDataset(root_dir='/content/Image',\n",
        "                            txt_COVID='/content/COVID-CT/Data-split/COVID/testCT_COVID.txt',\n",
        "                            txt_NonCOVID='/content/COVID-CT/Data-split/NonCOVID/testCT_NonCOVID.txt',\n",
        "                            transform= val_transformer)\n",
        "print(trainset.__len__())\n",
        "print(valset.__len__())\n",
        "print(testset.__len__())\n",
        "\n",
        "batchsize = 8\n",
        "\n",
        "train_loader = DataLoader(trainset, batch_size=batchsize, drop_last=False, shuffle=True)\n",
        "val_loader = DataLoader(valset, batch_size=batchsize, drop_last=False, shuffle=False)\n",
        "test_loader = DataLoader(testset, batch_size=batchsize, drop_last=False, shuffle=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "425\n",
            "118\n",
            "203\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q7shSxTnQc_m"
      },
      "source": [
        "def val(epoch):\n",
        "    \n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    results = []\n",
        "    \n",
        "    TP = 0\n",
        "    TN = 0\n",
        "    FN = 0\n",
        "    FP = 0\n",
        "    \n",
        "    \n",
        "    criteria = nn.CrossEntropyLoss()\n",
        "    # Don't update model\n",
        "    with torch.no_grad():\n",
        "        tpr_list = []\n",
        "        fpr_list = []\n",
        "        \n",
        "        predlist=[]\n",
        "        scorelist=[]\n",
        "        targetlist=[]\n",
        "        # Predict\n",
        "        for batch_index, batch_samples in enumerate(val_loader):\n",
        "            data, target = batch_samples['img'].to(device), batch_samples['label'].to(device)\n",
        "#            data = data[:, 0, :, :]\n",
        "#            data = data[:, None, :, :]\n",
        "            output = model(data)\n",
        "            \n",
        "            test_loss += criteria(output, target.long())\n",
        "            score = F.softmax(output, dim=1)\n",
        "            pred = output.argmax(dim=1, keepdim=True)\n",
        "#             print('target',target.long()[:, 2].view_as(pred))\n",
        "            correct += pred.eq(target.long().view_as(pred)).sum().item()\n",
        "            \n",
        "#             print(output[:,1].cpu().numpy())\n",
        "#             print((output[:,1]+output[:,0]).cpu().numpy())\n",
        "#             predcpu=(output[:,1].cpu().numpy())/((output[:,1]+output[:,0]).cpu().numpy())\n",
        "            targetcpu=target.long().cpu().numpy()\n",
        "            predlist=np.append(predlist, pred.cpu().numpy())\n",
        "            scorelist=np.append(scorelist, score.cpu().numpy()[:,1])\n",
        "            targetlist=np.append(targetlist,targetcpu)\n",
        "           \n",
        "          \n",
        "    return targetlist, scorelist, predlist\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zrXjTSLNQj2z"
      },
      "source": [
        "def test(epoch):\n",
        "    \n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    results = []\n",
        "    \n",
        "    TP = 0\n",
        "    TN = 0\n",
        "    FN = 0\n",
        "    FP = 0\n",
        "    \n",
        "    \n",
        "    criteria = nn.CrossEntropyLoss()\n",
        "    # Don't update model\n",
        "    with torch.no_grad():\n",
        "        tpr_list = []\n",
        "        fpr_list = []\n",
        "        \n",
        "        predlist=[]\n",
        "        scorelist=[]\n",
        "        targetlist=[]\n",
        "        # Predict\n",
        "        for batch_index, batch_samples in enumerate(test_loader):\n",
        "            data, target = batch_samples['img'].to(device), batch_samples['label'].to(device)\n",
        "#            data = data[:, 0, :, :]\n",
        "#            data = data[:, None, :, :]\n",
        "#             print(target)\n",
        "            output = model(data)\n",
        "            \n",
        "            test_loss += criteria(output, target.long())\n",
        "            score = F.softmax(output, dim=1)\n",
        "            pred = output.argmax(dim=1, keepdim=True)\n",
        "#             print('target',target.long()[:, 2].view_as(pred))\n",
        "            correct += pred.eq(target.long().view_as(pred)).sum().item()\n",
        "#             TP += ((pred == 1) & (target.long()[:, 2].view_as(pred).data == 1)).cpu().sum()\n",
        "#             TN += ((pred == 0) & (target.long()[:, 2].view_as(pred) == 0)).cpu().sum()\n",
        "# #             # FN    predict 0 label 1\n",
        "#             FN += ((pred == 0) & (target.long()[:, 2].view_as(pred) == 1)).cpu().sum()\n",
        "# #             # FP    predict 1 label 0\n",
        "#             FP += ((pred == 1) & (target.long()[:, 2].view_as(pred) == 0)).cpu().sum()\n",
        "#             print(TP,TN,FN,FP)\n",
        "            \n",
        "            \n",
        "#             print(output[:,1].cpu().numpy())\n",
        "#             print((output[:,1]+output[:,0]).cpu().numpy())\n",
        "#             predcpu=(output[:,1].cpu().numpy())/((output[:,1]+output[:,0]).cpu().numpy())\n",
        "            targetcpu=target.long().cpu().numpy()\n",
        "            predlist=np.append(predlist, pred.cpu().numpy())\n",
        "            scorelist=np.append(scorelist, score.cpu().numpy()[:,1])\n",
        "            targetlist=np.append(targetlist,targetcpu)\n",
        "           \n",
        "    return targetlist, scorelist, predlist\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gOmp8AXeFeLi"
      },
      "source": [
        "alpha = None\n",
        "device = 'cuda'\n",
        "def train(optimizer, epoch):\n",
        "    \n",
        "    model.train()\n",
        "    \n",
        "    train_loss = 0\n",
        "    train_correct = 0\n",
        "    \n",
        "    for batch_index, batch_samples in enumerate(train_loader):\n",
        "        \n",
        "        # move data to device\n",
        "        data, target = batch_samples['img'].to(device), batch_samples['label'].to(device)\n",
        "#       \n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        \n",
        "        criteria = nn.CrossEntropyLoss()\n",
        "        loss = criteria(output, target.long())\n",
        "#         loss = mixup_criterion(criteria, output, targets_a, targets_b, lam)\n",
        "        train_loss += criteria(output, target.long())\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        pred = output.argmax(dim=1, keepdim=True)\n",
        "        train_correct += pred.eq(target.long().view_as(pred)).sum().item()\n",
        "    \n",
        "        # Display progress and write to tensorboard\n",
        "        if batch_index % bs == 0:\n",
        "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tTrain Loss: {:.6f}'.format(\n",
        "                epoch, batch_index, len(train_loader),\n",
        "                100.0 * batch_index / len(train_loader), loss.item()/ bs))\n",
        "    \n",
        "    print('\\nTrain set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "        train_loss/len(train_loader.dataset), train_correct, len(train_loader.dataset),\n",
        "        100.0 * train_correct / len(train_loader.dataset)))\n",
        "    \n",
        "    f = open('modelb7_result/{}.txt'.format(modelname), 'a+')\n",
        "    f.write('\\nTrain set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "        train_loss/len(train_loader.dataset), train_correct, len(train_loader.dataset),\n",
        "        100.0 * train_correct / len(train_loader.dataset)))\n",
        "    f.write('\\n')\n",
        "    f.close()\n",
        "    \n",
        "    return train_loss/len(train_loader.dataset)\n",
        "\n",
        "\n",
        "\n",
        "# In[152]:\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vTQINxVfQnzQ"
      },
      "source": [
        "### Step 04: Install Efficientnet backbone"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vHHNAOqCFhrd",
        "outputId": "5a4587d3-cb10-43d6-ee82-ca91699d8437"
      },
      "source": [
        "!pip install efficientnet_pytorch"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting efficientnet_pytorch\n",
            "  Downloading https://files.pythonhosted.org/packages/4e/83/f9c5f44060f996279e474185ebcbd8dbd91179593bffb9abe3afa55d085b/efficientnet_pytorch-0.7.0.tar.gz\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from efficientnet_pytorch) (1.8.0+cu101)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->efficientnet_pytorch) (3.7.4.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch->efficientnet_pytorch) (1.19.5)\n",
            "Building wheels for collected packages: efficientnet-pytorch\n",
            "  Building wheel for efficientnet-pytorch (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for efficientnet-pytorch: filename=efficientnet_pytorch-0.7.0-cp37-none-any.whl size=16031 sha256=365a5a3e2ef26b30e277a06a40f62f8c27d1c07a662b25b2a7270cb3e0e6c247\n",
            "  Stored in directory: /root/.cache/pip/wheels/e9/c6/e1/7a808b26406239712cfce4b5ceeb67d9513ae32aa4b31445c6\n",
            "Successfully built efficientnet-pytorch\n",
            "Installing collected packages: efficientnet-pytorch\n",
            "Successfully installed efficientnet-pytorch-0.7.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZgtaowIfG5_Q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2d27e77b-045a-4e5e-fc58-93603b3dc2ac"
      },
      "source": [
        "!mkdir modelb7_result\n",
        "!mkdir train_10epochs_model_backup"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mkdir: cannot create directory â€˜train_10epochs_model_backupâ€™: File exists\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SckJlKcVEfau",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "fec7f169d8bc4dc088568f72b724301b",
            "184a6f2010494d6d9040cf4227c90c7c",
            "c4e0660969114c249a507e7a1d1001bc",
            "d0df5b4cae9b4b26886a720087ef8310",
            "8667cd3658b24e5d88d65d0908131fff",
            "f296bae49777489dbfd1786b7ba98f10",
            "2e301a00b48541d3ac1041db17613b4a",
            "5db16d3f5e654feaa7e749fa78af4759"
          ]
        },
        "outputId": "08e3f7ec-ca64-431b-8f48-1868b632af80"
      },
      "source": [
        "### efficientNet\n",
        "from efficientnet_pytorch import EfficientNet\n",
        "model = EfficientNet.from_pretrained('efficientnet-b7', num_classes=2)\n",
        "model = model.cuda()\n",
        "modelname = 'efficientNet-b7'\n",
        "\n",
        "bs = 10\n",
        "votenum = 10\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "r_list = []\n",
        "p_list = []\n",
        "acc_list = []\n",
        "AUC_list = []\n",
        "# TP = 0\n",
        "# TN = 0\n",
        "# FN = 0\n",
        "# FP = 0\n",
        "vote_pred = np.zeros(valset.__len__())\n",
        "vote_score = np.zeros(valset.__len__())\n",
        "\n",
        "#optimizer = optim.SGD(model.parameters(), lr=0.001, momentum = 0.9)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
        "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=10)\n",
        "#scheduler = StepLR(optimizer, step_size=1)\n",
        "\n",
        "total_epoch = 40\n",
        "cur_loss = 1000\n",
        "for epoch in range(1, total_epoch+1):\n",
        "    train_loss = train(optimizer, epoch)\n",
        "\n",
        "    #if (cur_loss < train_loss) and ((cur_loss - train_loss) < 0.000001):\n",
        "    #    break\n",
        "    #else:\n",
        "    #    cur_loss = train_loss\n",
        "\n",
        "    targetlist, scorelist, predlist = val(epoch)\n",
        "    print('target',targetlist)\n",
        "    print('score',scorelist)\n",
        "    print('predict',predlist)\n",
        "    vote_pred = vote_pred + predlist \n",
        "    vote_score = vote_score + scorelist \n",
        "\n",
        "    if epoch % votenum == 0:\n",
        "        \n",
        "        # major vote\n",
        "        vote_pred[vote_pred <= (votenum/2)] = 0\n",
        "        vote_pred[vote_pred > (votenum/2)] = 1\n",
        "        vote_score = vote_score/votenum\n",
        "        \n",
        "        print('vote_pred', vote_pred)\n",
        "        print('targetlist', targetlist)\n",
        "        TP = ((vote_pred == 1) & (targetlist == 1)).sum()\n",
        "        TN = ((vote_pred == 0) & (targetlist == 0)).sum()\n",
        "        FN = ((vote_pred == 0) & (targetlist == 1)).sum()\n",
        "        FP = ((vote_pred == 1) & (targetlist == 0)).sum()\n",
        "        \n",
        "        \n",
        "        print('TP=',TP,'TN=',TN,'FN=',FN,'FP=',FP)\n",
        "        print('TP+FP',TP+FP)\n",
        "        p = TP / (TP + FP)\n",
        "        print('precision',p)\n",
        "        p = TP / (TP + FP)\n",
        "        r = TP / (TP + FN)\n",
        "        print('recall',r)\n",
        "        F1 = 2 * r * p / (r + p)\n",
        "        acc = (TP + TN) / (TP + TN + FP + FN)\n",
        "        print('F1',F1)\n",
        "        print('acc',acc)\n",
        "        AUC = roc_auc_score(targetlist, vote_score)\n",
        "        print('AUCp', roc_auc_score(targetlist, vote_pred))\n",
        "        print('AUC', AUC)\n",
        "        \n",
        "        \n",
        "        \n",
        "#         if epoch == total_epoch:\n",
        "        torch.save(model.state_dict(), \"train_10epochs_model_backup/{}.pth\".format(modelname))  \n",
        "        \n",
        "        vote_pred = np.zeros(valset.__len__())\n",
        "        vote_score = np.zeros(valset.__len__())\n",
        "        print('\\n The epoch is {}, average recall: {:.4f}, average precision: {:.4f},average F1: {:.4f}, average accuracy: {:.4f}, average AUC: {:.4f}'.format(\n",
        "        epoch, r, p, F1, acc, AUC))\n",
        "\n",
        "        f = open('/content/modelb7_result/{}.txt'.format(modelname), 'a+')\n",
        "        f.write('\\n The epoch is {}, average recall: {:.4f}, average precision: {:.4f},average F1: {:.4f}, average accuracy: {:.4f}, average AUC: {:.4f}'.format(\n",
        "        epoch, r, p, F1, acc, AUC))\n",
        "        f.close()\n",
        "\n",
        "\n",
        "# In[145]:\n",
        "\n",
        "\n",
        "\n",
        "    "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading: \"https://github.com/lukemelas/EfficientNet-PyTorch/releases/download/1.0/efficientnet-b7-dcc49843.pth\" to /root/.cache/torch/hub/checkpoints/efficientnet-b7-dcc49843.pth\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fec7f169d8bc4dc088568f72b724301b",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=266860719.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Loaded pretrained weights for efficientnet-b7\n",
            "Train Epoch: 1 [0/54 (0%)]\tTrain Loss: 0.070402\n",
            "Train Epoch: 1 [10/54 (19%)]\tTrain Loss: 0.067073\n",
            "Train Epoch: 1 [20/54 (37%)]\tTrain Loss: 0.075941\n",
            "Train Epoch: 1 [30/54 (56%)]\tTrain Loss: 0.061829\n",
            "Train Epoch: 1 [40/54 (74%)]\tTrain Loss: 0.041016\n",
            "Train Epoch: 1 [50/54 (93%)]\tTrain Loss: 0.047014\n",
            "\n",
            "Train set: Average loss: 0.0767, Accuracy: 287/425 (68%)\n",
            "\n",
            "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            "score [0.49373552 0.55393398 0.63021719 0.63362116 0.4489952  0.58370119\n",
            " 0.68258911 0.66679639 0.54642129 0.38953    0.5763191  0.29776391\n",
            " 0.52135319 0.51685613 0.50404251 0.53674889 0.40945753 0.5162518\n",
            " 0.53720677 0.43842733 0.63654262 0.53132582 0.5866695  0.70665425\n",
            " 0.63360345 0.67406511 0.70504427 0.75436568 0.59184426 0.50003046\n",
            " 0.75901484 0.80988145 0.63562274 0.43742198 0.37859634 0.47169772\n",
            " 0.61763513 0.65494424 0.7104671  0.60375428 0.53120869 0.72452122\n",
            " 0.60043412 0.51669502 0.59833962 0.43683356 0.59834689 0.6339488\n",
            " 0.67573225 0.66651291 0.75263631 0.4182958  0.4527263  0.36133984\n",
            " 0.65086907 0.49509025 0.53535146 0.29126304 0.35422736 0.54664755\n",
            " 0.80080044 0.54518431 0.62222904 0.6636197  0.74841142 0.67793322\n",
            " 0.58260041 0.89391923 0.68103844 0.71054649 0.52293801 0.52826691\n",
            " 0.72641677 0.67533255 0.73208392 0.74504179 0.73618031 0.78581041\n",
            " 0.78347623 0.82121521 0.68750519 0.65020961 0.78122455 0.6131562\n",
            " 0.55030954 0.7342813  0.70304465 0.62038326 0.64663172 0.65894437\n",
            " 0.58264315 0.50798845 0.46144512 0.57497412 0.70951343 0.66374725\n",
            " 0.54293787 0.59032232 0.71561807 0.68657631 0.38753071 0.55756664\n",
            " 0.62907803 0.43062204 0.51140136 0.50395679 0.5664013  0.65936065\n",
            " 0.48813394 0.59590775 0.47930503 0.66095263 0.63168854 0.69169617\n",
            " 0.70811969 0.84026444 0.73220319 0.6243093 ]\n",
            "predict [0. 1. 1. 1. 0. 1. 1. 1. 1. 0. 1. 0. 1. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1.\n",
            " 1. 1. 1. 0. 0. 0. 1. 0. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1.\n",
            " 1. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 1. 0. 1. 0. 1. 1. 1. 1. 1. 1. 1.]\n",
            "Train Epoch: 2 [0/54 (0%)]\tTrain Loss: 0.046783\n",
            "Train Epoch: 2 [10/54 (19%)]\tTrain Loss: 0.047906\n",
            "Train Epoch: 2 [20/54 (37%)]\tTrain Loss: 0.046581\n",
            "Train Epoch: 2 [30/54 (56%)]\tTrain Loss: 0.021187\n",
            "Train Epoch: 2 [40/54 (74%)]\tTrain Loss: 0.040263\n",
            "Train Epoch: 2 [50/54 (93%)]\tTrain Loss: 0.023494\n",
            "\n",
            "Train set: Average loss: 0.0517, Accuracy: 356/425 (84%)\n",
            "\n",
            "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            "score [0.39522865 0.62769306 0.7250278  0.76684719 0.34634823 0.39486676\n",
            " 0.75525033 0.75168049 0.51629728 0.30391085 0.74993491 0.18517019\n",
            " 0.67420316 0.4480491  0.59061211 0.60471064 0.38331586 0.6405614\n",
            " 0.42356548 0.30829483 0.57559413 0.605672   0.59207565 0.94865465\n",
            " 0.79020905 0.68166834 0.79627031 0.78200978 0.37684491 0.3066138\n",
            " 0.62696791 0.52793795 0.62516481 0.35561949 0.25118646 0.4352738\n",
            " 0.44868711 0.83474702 0.79278493 0.79729348 0.69406664 0.86490053\n",
            " 0.52539045 0.71533233 0.60653263 0.35565814 0.54944026 0.55214733\n",
            " 0.86956549 0.96002489 0.92461473 0.3864933  0.61410534 0.32518572\n",
            " 0.72092026 0.37644941 0.93157804 0.15797894 0.15886971 0.71098125\n",
            " 0.98700768 0.95641172 0.9612205  0.93062508 0.58945554 0.76482868\n",
            " 0.57843852 0.98559278 0.91634703 0.86495531 0.42818972 0.52480227\n",
            " 0.65952003 0.58513063 0.78493065 0.7571339  0.61820954 0.88479453\n",
            " 0.48120645 0.78905421 0.72892135 0.7096048  0.86140603 0.84102249\n",
            " 0.85710937 0.80577219 0.86622602 0.63963354 0.51247537 0.74873245\n",
            " 0.75061029 0.83966142 0.62892574 0.84094298 0.96025157 0.84689891\n",
            " 0.7953558  0.72759444 0.78128809 0.96830976 0.32068187 0.51107275\n",
            " 0.76298547 0.57409728 0.59517258 0.59632158 0.83336222 0.75566429\n",
            " 0.66464716 0.44607344 0.62272424 0.81849027 0.7271499  0.64034396\n",
            " 0.88311142 0.89289439 0.83093387 0.77207202]\n",
            "predict [0. 1. 1. 1. 0. 0. 1. 1. 1. 0. 1. 0. 1. 0. 1. 1. 0. 1. 0. 0. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 0. 0. 1. 1. 1. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1.\n",
            " 1. 1. 1. 0. 1. 0. 1. 0. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1.\n",
            " 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            "Train Epoch: 3 [0/54 (0%)]\tTrain Loss: 0.039716\n",
            "Train Epoch: 3 [10/54 (19%)]\tTrain Loss: 0.024826\n",
            "Train Epoch: 3 [20/54 (37%)]\tTrain Loss: 0.014844\n",
            "Train Epoch: 3 [30/54 (56%)]\tTrain Loss: 0.024687\n",
            "Train Epoch: 3 [40/54 (74%)]\tTrain Loss: 0.024375\n",
            "Train Epoch: 3 [50/54 (93%)]\tTrain Loss: 0.059409\n",
            "\n",
            "Train set: Average loss: 0.0361, Accuracy: 375/425 (88%)\n",
            "\n",
            "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            "score [0.15337312 0.26488781 0.1557402  0.46434325 0.04985247 0.34907919\n",
            " 0.63904768 0.85023648 0.31778067 0.15670273 0.10555408 0.11669644\n",
            " 0.25675336 0.08639786 0.22260442 0.05910101 0.04875048 0.70396161\n",
            " 0.65519357 0.22158261 0.72828054 0.55317163 0.61610132 0.99151057\n",
            " 0.66199201 0.77778459 0.91180086 0.54744858 0.13031667 0.05427643\n",
            " 0.73325193 0.79600507 0.68050909 0.02525714 0.02140475 0.10953297\n",
            " 0.05360871 0.83761382 0.76389784 0.62331349 0.73456818 0.84958684\n",
            " 0.56234074 0.76677346 0.68937999 0.40886527 0.85747838 0.88040632\n",
            " 0.92390364 0.93310893 0.93778753 0.01101229 0.10706524 0.0055724\n",
            " 0.74993736 0.46272668 0.76831234 0.01870079 0.01961461 0.63947058\n",
            " 0.93217111 0.94064498 0.91990066 0.92428273 0.71593159 0.77746725\n",
            " 0.5752604  0.9749018  0.92269373 0.95072865 0.26123285 0.37094694\n",
            " 0.91237664 0.82229942 0.86822534 0.84604609 0.54538405 0.82945877\n",
            " 0.68930471 0.90497667 0.91485268 0.92617208 0.91304445 0.823008\n",
            " 0.60432535 0.92340511 0.87775868 0.77147877 0.71300191 0.88807309\n",
            " 0.72917902 0.87557185 0.64346004 0.91774631 0.97703427 0.61445975\n",
            " 0.56216681 0.7319532  0.83217508 0.93509662 0.08240432 0.69237411\n",
            " 0.61945611 0.48927292 0.61073291 0.81402308 0.92473811 0.8479917\n",
            " 0.68326932 0.3876794  0.59645307 0.9113245  0.91677606 0.7937364\n",
            " 0.46341854 0.57722574 0.93944567 0.70196885]\n",
            "predict [0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 0. 0. 1. 1. 1. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1.\n",
            " 1. 1. 1. 0. 0. 0. 1. 0. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 0. 1. 1. 1.]\n",
            "Train Epoch: 4 [0/54 (0%)]\tTrain Loss: 0.032992\n",
            "Train Epoch: 4 [10/54 (19%)]\tTrain Loss: 0.082125\n",
            "Train Epoch: 4 [20/54 (37%)]\tTrain Loss: 0.007458\n",
            "Train Epoch: 4 [30/54 (56%)]\tTrain Loss: 0.011943\n",
            "Train Epoch: 4 [40/54 (74%)]\tTrain Loss: 0.013875\n",
            "Train Epoch: 4 [50/54 (93%)]\tTrain Loss: 0.004341\n",
            "\n",
            "Train set: Average loss: 0.0302, Accuracy: 382/425 (90%)\n",
            "\n",
            "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            "score [0.17598124 0.35456532 0.10068183 0.39607802 0.04264831 0.15867381\n",
            " 0.75954586 0.81504273 0.14843629 0.05632186 0.11745541 0.05781806\n",
            " 0.38792655 0.11426197 0.42697275 0.02529191 0.05371839 0.27721816\n",
            " 0.59552383 0.02561138 0.44132122 0.78379899 0.88859081 0.99907315\n",
            " 0.85407794 0.9379288  0.99454051 0.05428673 0.0423662  0.07498184\n",
            " 0.25665894 0.35354343 0.6254462  0.02938221 0.02326289 0.06116198\n",
            " 0.10143507 0.88549429 0.55368757 0.27522534 0.70312661 0.47575313\n",
            " 0.47946936 0.61531615 0.03722052 0.0486238  0.93035704 0.86621785\n",
            " 0.96465433 0.97874397 0.99010617 0.0098924  0.01106374 0.00799756\n",
            " 0.46028939 0.02074549 0.83216828 0.0095585  0.00997128 0.02266915\n",
            " 0.99817622 0.99873751 0.9977569  0.997971   0.10028553 0.44209346\n",
            " 0.56207901 0.99923742 0.97202629 0.99797529 0.62009978 0.47699904\n",
            " 0.90342689 0.91721791 0.38757131 0.66287839 0.60395914 0.96759248\n",
            " 0.80347455 0.99363577 0.94696563 0.45897722 0.99593985 0.55287367\n",
            " 0.29331142 0.97763032 0.98035473 0.82777041 0.50154591 0.90106297\n",
            " 0.97151977 0.94748729 0.22989078 0.99555749 0.99863285 0.81490201\n",
            " 0.18809225 0.71335304 0.96434295 0.97128254 0.29135603 0.44490972\n",
            " 0.12631033 0.44109836 0.50211513 0.30931759 0.97023678 0.89031619\n",
            " 0.34507343 0.58584511 0.3490268  0.8998875  0.96060777 0.99133462\n",
            " 0.10701264 0.14705758 0.99808335 0.67676514]\n",
            "predict [0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1. 1.\n",
            " 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 0. 1. 0. 0. 1. 0. 0. 1. 1.\n",
            " 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 1. 1. 0. 0. 1. 1. 1. 1. 1. 0.\n",
            " 1. 1. 0. 1. 1. 1. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1.\n",
            " 0. 1. 1. 1. 0. 0. 0. 0. 1. 0. 1. 1. 0. 1. 0. 1. 1. 1. 0. 0. 1. 1.]\n",
            "Train Epoch: 5 [0/54 (0%)]\tTrain Loss: 0.038663\n",
            "Train Epoch: 5 [10/54 (19%)]\tTrain Loss: 0.018043\n",
            "Train Epoch: 5 [20/54 (37%)]\tTrain Loss: 0.015749\n",
            "Train Epoch: 5 [30/54 (56%)]\tTrain Loss: 0.008645\n",
            "Train Epoch: 5 [40/54 (74%)]\tTrain Loss: 0.017675\n",
            "Train Epoch: 5 [50/54 (93%)]\tTrain Loss: 0.010306\n",
            "\n",
            "Train set: Average loss: 0.0275, Accuracy: 388/425 (91%)\n",
            "\n",
            "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            "score [0.05237018 0.11591066 0.02769874 0.35995895 0.00303741 0.03937038\n",
            " 0.54070705 0.09534115 0.03591356 0.03274471 0.03890968 0.02416467\n",
            " 0.07750644 0.27964306 0.42590326 0.00186654 0.0284428  0.6311025\n",
            " 0.91787076 0.01836171 0.28756854 0.84301746 0.82369697 0.99971932\n",
            " 0.77457029 0.96145409 0.99641871 0.18025181 0.01799172 0.02797732\n",
            " 0.60180998 0.38989234 0.31207341 0.00950294 0.00251296 0.03803348\n",
            " 0.04870867 0.80185401 0.70525646 0.05182187 0.2342715  0.17453994\n",
            " 0.38121554 0.06920084 0.01115813 0.00504849 0.30242607 0.17445971\n",
            " 0.73244029 0.95136744 0.43046466 0.00262019 0.00356455 0.00124071\n",
            " 0.1207597  0.00106853 0.60652232 0.00201399 0.00335712 0.00369946\n",
            " 0.99864537 0.99977595 0.99766052 0.99792576 0.07904825 0.5758487\n",
            " 0.84842736 0.99909747 0.90503234 0.17850639 0.0829377  0.1685307\n",
            " 0.82483828 0.55018473 0.25389194 0.89375925 0.75916576 0.99126256\n",
            " 0.77342492 0.77478629 0.79006881 0.24112937 0.98518026 0.86097258\n",
            " 0.08532761 0.69057757 0.97871727 0.76259047 0.88010806 0.97008836\n",
            " 0.99449533 0.51474184 0.14192581 0.99582702 0.99859816 0.91582829\n",
            " 0.12170157 0.63235122 0.92154026 0.72470224 0.32307121 0.33857399\n",
            " 0.17551957 0.08142947 0.02579952 0.12440218 0.72887671 0.33853921\n",
            " 0.2643421  0.04102551 0.11503535 0.70360392 0.95447296 0.9627291\n",
            " 0.08796579 0.57220536 0.99927062 0.82100332]\n",
            "predict [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 1. 1.\n",
            " 1. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 1. 1. 0. 1. 1. 1. 1. 0. 0. 0.\n",
            " 1. 1. 0. 1. 1. 1. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1.\n",
            " 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 1. 0. 1. 1. 1.]\n",
            "Train Epoch: 6 [0/54 (0%)]\tTrain Loss: 0.002862\n",
            "Train Epoch: 6 [10/54 (19%)]\tTrain Loss: 0.002680\n",
            "Train Epoch: 6 [20/54 (37%)]\tTrain Loss: 0.045673\n",
            "Train Epoch: 6 [30/54 (56%)]\tTrain Loss: 0.002359\n",
            "Train Epoch: 6 [40/54 (74%)]\tTrain Loss: 0.060315\n",
            "Train Epoch: 6 [50/54 (93%)]\tTrain Loss: 0.003087\n",
            "\n",
            "Train set: Average loss: 0.0237, Accuracy: 399/425 (94%)\n",
            "\n",
            "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            "score [1.54199451e-01 2.88294733e-01 5.57217933e-02 7.77101457e-01\n",
            " 1.13495132e-02 5.05096093e-02 8.12049448e-01 8.30502510e-01\n",
            " 1.81459740e-01 2.04159319e-02 2.38467708e-01 9.51861870e-03\n",
            " 6.36317909e-01 3.51210356e-01 8.51159394e-01 1.72243882e-02\n",
            " 6.79938719e-02 5.10580480e-01 8.58084023e-01 7.94194713e-02\n",
            " 6.38023674e-01 7.14593291e-01 7.93360472e-01 9.95046854e-01\n",
            " 7.57674217e-01 9.00678337e-01 9.75732565e-01 8.12354565e-01\n",
            " 9.37666595e-02 9.82146412e-02 7.85040855e-01 8.57758045e-01\n",
            " 7.03478456e-01 8.43426120e-03 1.27247663e-03 1.57702088e-01\n",
            " 9.59925056e-02 9.86964822e-01 8.14733982e-01 8.27565253e-01\n",
            " 9.30246830e-01 8.92785192e-01 5.99763691e-01 8.97354424e-01\n",
            " 7.41814554e-01 7.74133131e-02 8.73864412e-01 6.45485818e-01\n",
            " 9.51750159e-01 9.11297083e-01 9.32007670e-01 2.82629370e-03\n",
            " 1.13417336e-03 1.34670269e-03 4.06380780e-02 1.04951416e-03\n",
            " 3.60680580e-01 5.54521917e-04 1.38806608e-02 2.25452352e-02\n",
            " 9.93250012e-01 9.94082391e-01 9.86735702e-01 9.89653647e-01\n",
            " 8.10183644e-01 9.82461751e-01 9.08604980e-01 9.95353937e-01\n",
            " 9.93133903e-01 9.15670335e-01 5.24947703e-01 6.81579649e-01\n",
            " 9.90660965e-01 8.37208331e-01 8.71371388e-01 9.91060317e-01\n",
            " 9.41255033e-01 9.70342398e-01 9.54755187e-01 9.64088261e-01\n",
            " 9.79881287e-01 9.80264544e-01 9.88757789e-01 7.48299181e-01\n",
            " 3.54982793e-01 8.10606956e-01 9.39474344e-01 7.64053524e-01\n",
            " 8.54195058e-01 9.44843829e-01 8.51159871e-01 9.58248854e-01\n",
            " 5.87237656e-01 9.76399004e-01 9.95836854e-01 9.08783734e-01\n",
            " 1.96359664e-01 1.07746802e-01 4.05416936e-01 9.66874957e-01\n",
            " 1.50965750e-01 5.35226107e-01 7.83795118e-01 9.54497680e-02\n",
            " 2.58773621e-02 1.80533975e-01 7.69553125e-01 6.83735549e-01\n",
            " 9.16137844e-02 3.71872991e-01 2.88534969e-01 3.14929396e-01\n",
            " 9.81319904e-01 9.94302988e-01 7.32856333e-01 7.10864544e-01\n",
            " 9.98735607e-01 9.10097778e-01]\n",
            "predict [0. 0. 0. 1. 0. 0. 1. 1. 0. 0. 0. 0. 1. 0. 1. 0. 0. 1. 1. 0. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 0. 0. 1. 1. 1. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1.\n",
            " 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1.]\n",
            "Train Epoch: 7 [0/54 (0%)]\tTrain Loss: 0.004176\n",
            "Train Epoch: 7 [10/54 (19%)]\tTrain Loss: 0.019643\n",
            "Train Epoch: 7 [20/54 (37%)]\tTrain Loss: 0.028163\n",
            "Train Epoch: 7 [30/54 (56%)]\tTrain Loss: 0.007161\n",
            "Train Epoch: 7 [40/54 (74%)]\tTrain Loss: 0.018961\n",
            "Train Epoch: 7 [50/54 (93%)]\tTrain Loss: 0.005644\n",
            "\n",
            "Train set: Average loss: 0.0176, Accuracy: 403/425 (95%)\n",
            "\n",
            "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            "score [5.28421998e-02 1.33304819e-01 3.42967659e-02 1.40030071e-01\n",
            " 7.58255410e-05 2.42084567e-03 6.98253870e-01 1.90295875e-01\n",
            " 1.10908626e-02 1.53539972e-02 1.31024539e-01 6.11430872e-03\n",
            " 1.35483742e-01 2.64697075e-02 4.13920850e-01 1.84391283e-05\n",
            " 3.66182392e-03 5.29838093e-02 8.25886369e-01 1.37157384e-02\n",
            " 3.32415968e-01 3.13640624e-01 1.45538047e-01 9.99852061e-01\n",
            " 4.90190506e-01 4.10518527e-01 9.91910040e-01 1.90814286e-01\n",
            " 2.99458578e-02 3.03146914e-02 6.05416119e-01 7.41521001e-01\n",
            " 3.81245613e-01 4.03491926e-04 1.27961239e-05 7.09147099e-03\n",
            " 2.46188473e-02 4.42181826e-01 2.15490595e-01 6.31171167e-02\n",
            " 3.65557343e-01 2.31766254e-01 3.75718623e-01 4.29034196e-02\n",
            " 1.59911960e-02 3.75994369e-02 8.44191372e-01 5.65481663e-01\n",
            " 9.83545125e-01 8.21172714e-01 9.93873119e-01 8.32521473e-04\n",
            " 2.54147826e-03 1.01517956e-03 1.77961662e-01 3.34083568e-03\n",
            " 5.55290282e-01 8.26746691e-04 7.02516455e-03 8.48676136e-04\n",
            " 9.98910069e-01 9.99521732e-01 9.99130785e-01 9.96102691e-01\n",
            " 4.78431098e-02 6.50718987e-01 4.67966557e-01 9.96661901e-01\n",
            " 9.89283800e-01 9.37068284e-01 5.59928954e-01 7.68568039e-01\n",
            " 9.91762400e-01 4.98186141e-01 6.36653543e-01 9.54680204e-01\n",
            " 9.38745141e-01 9.72928524e-01 9.66614246e-01 7.93719828e-01\n",
            " 9.77009892e-01 7.07483292e-01 9.99827325e-01 8.91507789e-02\n",
            " 2.23851413e-01 9.81606841e-01 9.87689137e-01 8.92993152e-01\n",
            " 7.41756976e-01 9.11536396e-01 8.53288352e-01 6.28587484e-01\n",
            " 1.57142073e-01 9.66725051e-01 9.61026728e-01 8.70214641e-01\n",
            " 1.97175950e-01 1.94441110e-01 6.89839244e-01 7.21392751e-01\n",
            " 6.42720535e-02 3.01052988e-01 2.03534096e-01 3.86043452e-02\n",
            " 2.26283800e-02 2.33800650e-01 9.67118084e-01 9.29199830e-02\n",
            " 5.35964221e-03 1.89055875e-02 3.79079312e-01 8.78559947e-02\n",
            " 6.82527184e-01 9.65589821e-01 1.94161698e-01 2.72292256e-01\n",
            " 9.99453008e-01 9.36090469e-01]\n",
            "predict [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1.\n",
            " 0. 0. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1.\n",
            " 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 1. 1. 0. 1. 0. 1. 1. 1. 1. 1.\n",
            " 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1.\n",
            " 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 1.]\n",
            "Train Epoch: 8 [0/54 (0%)]\tTrain Loss: 0.003568\n",
            "Train Epoch: 8 [10/54 (19%)]\tTrain Loss: 0.005332\n",
            "Train Epoch: 8 [20/54 (37%)]\tTrain Loss: 0.004632\n",
            "Train Epoch: 8 [30/54 (56%)]\tTrain Loss: 0.023584\n",
            "Train Epoch: 8 [40/54 (74%)]\tTrain Loss: 0.151878\n",
            "Train Epoch: 8 [50/54 (93%)]\tTrain Loss: 0.003245\n",
            "\n",
            "Train set: Average loss: 0.0176, Accuracy: 411/425 (97%)\n",
            "\n",
            "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            "score [1.32481065e-02 1.97647467e-01 7.99933076e-02 6.04130735e-04\n",
            " 2.20236307e-05 8.40931825e-05 7.02802837e-01 2.25768760e-02\n",
            " 2.07485701e-03 9.37346667e-02 3.61812681e-01 2.15452984e-02\n",
            " 5.03970802e-01 2.23628893e-01 3.52111936e-01 8.51370987e-06\n",
            " 1.10929497e-02 7.80753605e-03 3.04643750e-01 1.44884869e-01\n",
            " 2.51716822e-01 9.63082537e-02 4.05504525e-01 9.98882592e-01\n",
            " 3.94922554e-01 8.44268262e-01 9.59752440e-01 5.27434826e-01\n",
            " 1.75448686e-01 1.59860045e-01 5.29930294e-01 8.12923968e-01\n",
            " 1.06523909e-01 3.74157727e-03 5.38569235e-04 3.90613452e-02\n",
            " 7.73116797e-02 9.53449979e-02 2.96474785e-01 1.75820991e-01\n",
            " 3.82181555e-01 3.57824922e-01 2.89661735e-01 2.11031020e-01\n",
            " 3.34696889e-01 3.28825176e-01 6.16141617e-01 5.64493656e-01\n",
            " 7.82024682e-01 6.83970392e-01 8.10056984e-01 1.31313899e-03\n",
            " 2.50482932e-04 3.51859984e-04 1.64862633e-01 1.64695506e-04\n",
            " 4.12041172e-02 6.37744670e-04 1.99313182e-03 2.32921890e-03\n",
            " 9.57086563e-01 9.38082457e-01 9.68715549e-01 9.32752073e-01\n",
            " 1.25985742e-01 6.32539332e-01 2.17091873e-01 9.98852849e-01\n",
            " 9.64622855e-01 4.13204461e-01 1.79073691e-01 4.38520283e-01\n",
            " 9.67117071e-01 5.24000645e-01 6.88337386e-01 9.18938577e-01\n",
            " 9.61377621e-01 9.14911926e-01 9.79909003e-01 8.07187796e-01\n",
            " 9.19051051e-01 6.82711244e-01 9.98694122e-01 2.38378439e-02\n",
            " 1.53847143e-01 8.11009109e-01 9.76517677e-01 8.89534950e-01\n",
            " 9.28745747e-01 9.32690620e-01 9.71090496e-01 2.56041855e-01\n",
            " 1.14766276e-02 9.70785499e-01 9.80959773e-01 9.69523370e-01\n",
            " 2.05531329e-01 1.46331415e-01 4.32259262e-01 7.72102296e-01\n",
            " 1.15611888e-01 1.51254460e-01 8.30959022e-01 2.05380581e-02\n",
            " 4.78672981e-03 1.36885643e-01 7.99688399e-01 2.45885570e-02\n",
            " 4.49911039e-03 6.93566864e-03 7.54537582e-02 1.60966784e-01\n",
            " 6.07473016e-01 9.88777697e-01 1.98242605e-01 5.88373244e-01\n",
            " 9.97786045e-01 9.24273252e-01]\n",
            "predict [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.\n",
            " 0. 1. 1. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1.\n",
            " 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 0. 1. 0. 1. 1. 0. 0. 0.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1.\n",
            " 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 1. 0. 1. 1. 1.]\n",
            "Train Epoch: 9 [0/54 (0%)]\tTrain Loss: 0.053154\n",
            "Train Epoch: 9 [10/54 (19%)]\tTrain Loss: 0.051844\n",
            "Train Epoch: 9 [20/54 (37%)]\tTrain Loss: 0.008629\n",
            "Train Epoch: 9 [30/54 (56%)]\tTrain Loss: 0.006108\n",
            "Train Epoch: 9 [40/54 (74%)]\tTrain Loss: 0.000255\n",
            "Train Epoch: 9 [50/54 (93%)]\tTrain Loss: 0.002372\n",
            "\n",
            "Train set: Average loss: 0.0151, Accuracy: 412/425 (97%)\n",
            "\n",
            "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            "score [1.28618005e-04 5.41193821e-02 2.05429941e-02 3.71260941e-02\n",
            " 1.84161021e-04 9.02877946e-05 3.03565532e-01 2.34899998e-01\n",
            " 9.24084429e-03 1.30099177e-01 3.23756605e-01 4.87624072e-02\n",
            " 5.22941649e-01 2.80888706e-01 5.63029826e-01 9.62095146e-05\n",
            " 1.33223133e-02 7.55238757e-02 3.47506076e-01 5.26963882e-02\n",
            " 3.22860420e-01 2.58552372e-01 1.81016177e-01 9.99929309e-01\n",
            " 6.83753312e-01 8.95444274e-01 9.97812629e-01 2.91154116e-01\n",
            " 6.62670359e-02 8.33827779e-02 7.16951728e-01 8.61650407e-01\n",
            " 2.42928356e-01 7.01377518e-04 1.11434225e-03 5.91372587e-02\n",
            " 8.40045884e-02 6.37804449e-01 3.55897248e-01 2.95540661e-01\n",
            " 4.27380979e-01 7.35818744e-01 2.65027940e-01 1.44327149e-01\n",
            " 4.60274249e-01 4.05094951e-01 8.93879056e-01 7.89537013e-01\n",
            " 8.78952801e-01 6.52536631e-01 9.30827796e-01 8.05716700e-05\n",
            " 5.72074714e-06 1.06475971e-04 6.60909340e-02 1.47467767e-06\n",
            " 2.38178596e-02 2.08806242e-07 4.80067963e-03 7.53203814e-04\n",
            " 9.73316848e-01 9.40925002e-01 9.81289446e-01 9.86662686e-01\n",
            " 1.06704213e-01 9.73264217e-01 5.61985373e-01 9.99896646e-01\n",
            " 9.74425495e-01 7.04855561e-01 5.05763181e-02 3.74775857e-01\n",
            " 9.98982728e-01 6.76124275e-01 6.35031641e-01 9.70171809e-01\n",
            " 9.56456780e-01 9.70641613e-01 9.97499049e-01 9.71854866e-01\n",
            " 9.97013092e-01 9.44901764e-01 9.99851227e-01 5.20468354e-01\n",
            " 3.26042145e-01 9.31926429e-01 9.89780903e-01 8.95810127e-01\n",
            " 9.53563392e-01 9.99587953e-01 9.93149102e-01 6.02260888e-01\n",
            " 1.34520590e-01 9.78971958e-01 9.97980177e-01 9.44560707e-01\n",
            " 3.31396088e-02 1.82885528e-01 6.15458548e-01 9.45079327e-01\n",
            " 1.15579687e-01 2.00036362e-01 8.69664013e-01 2.28811488e-01\n",
            " 2.69845370e-02 1.05995856e-01 6.95431113e-01 1.83119789e-01\n",
            " 1.18281199e-02 8.17221217e-03 1.05823226e-01 2.19517782e-01\n",
            " 9.16415870e-01 9.99706328e-01 5.98774016e-01 8.75060678e-01\n",
            " 9.98621106e-01 9.03614044e-01]\n",
            "predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1.\n",
            " 1. 1. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1.\n",
            " 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 0. 0.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1.\n",
            " 0. 0. 1. 1. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1.]\n",
            "Train Epoch: 10 [0/54 (0%)]\tTrain Loss: 0.017477\n",
            "Train Epoch: 10 [10/54 (19%)]\tTrain Loss: 0.042107\n",
            "Train Epoch: 10 [20/54 (37%)]\tTrain Loss: 0.001142\n",
            "Train Epoch: 10 [30/54 (56%)]\tTrain Loss: 0.000481\n",
            "Train Epoch: 10 [40/54 (74%)]\tTrain Loss: 0.004043\n",
            "Train Epoch: 10 [50/54 (93%)]\tTrain Loss: 0.002024\n",
            "\n",
            "Train set: Average loss: 0.0134, Accuracy: 409/425 (96%)\n",
            "\n",
            "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            "score [2.19187071e-03 6.51371062e-01 9.44150612e-02 1.71235025e-01\n",
            " 2.93158228e-03 5.37334199e-06 9.53647077e-01 3.34063739e-01\n",
            " 3.33032897e-03 1.85258687e-01 2.61811614e-01 2.91102529e-02\n",
            " 2.78121978e-01 1.03865996e-01 5.11845171e-01 4.57981150e-06\n",
            " 5.18461969e-03 5.82633615e-01 8.21986258e-01 4.01320159e-02\n",
            " 3.79806012e-01 8.58504236e-01 8.22560787e-01 9.99999523e-01\n",
            " 9.60123420e-01 9.96750236e-01 9.99991775e-01 2.27724835e-02\n",
            " 8.31665192e-03 2.74949819e-02 3.39533508e-01 9.09302473e-01\n",
            " 5.18936515e-01 2.29402125e-04 4.36591145e-05 1.75725948e-02\n",
            " 5.04962616e-02 9.17718947e-01 5.47929287e-01 3.88406664e-01\n",
            " 5.23061633e-01 8.65555823e-01 3.35594058e-01 1.90150246e-01\n",
            " 9.13311005e-01 3.38456571e-01 9.92323160e-01 9.24990654e-01\n",
            " 9.89556372e-01 8.15957844e-01 9.99117315e-01 5.89044590e-04\n",
            " 7.66903341e-09 9.52434948e-06 9.14721414e-02 4.12844145e-07\n",
            " 5.37489429e-02 1.04851097e-07 1.51238660e-03 1.33094529e-03\n",
            " 9.99396920e-01 9.99987245e-01 9.99670506e-01 9.99579608e-01\n",
            " 6.47988796e-01 9.93405700e-01 8.27125788e-01 1.00000000e+00\n",
            " 9.99437511e-01 9.91514027e-01 1.47538453e-01 5.50595820e-01\n",
            " 9.99942064e-01 4.16530997e-01 5.15169084e-01 9.96482491e-01\n",
            " 9.99843717e-01 9.99939322e-01 9.97219443e-01 9.99555171e-01\n",
            " 9.99976277e-01 9.99971747e-01 9.99999523e-01 9.55447912e-01\n",
            " 6.39432728e-01 9.99504566e-01 9.99865413e-01 9.98945892e-01\n",
            " 7.98630655e-01 9.99998331e-01 9.99166489e-01 9.00275707e-01\n",
            " 4.07357514e-01 9.95986998e-01 9.97062743e-01 9.91093576e-01\n",
            " 6.73312098e-02 7.75741100e-01 8.90732586e-01 9.99898672e-01\n",
            " 2.85385221e-01 2.84099728e-01 2.38808885e-01 3.36795777e-01\n",
            " 1.14104703e-01 7.54257262e-01 9.50311124e-01 8.81300941e-02\n",
            " 6.40528500e-02 1.32713854e-01 1.55651823e-01 3.17344993e-01\n",
            " 9.99339998e-01 9.99994159e-01 9.02579069e-01 9.98802423e-01\n",
            " 9.99890924e-01 9.99984741e-01]\n",
            "predict [0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1. 0. 0. 1. 1. 1.\n",
            " 1. 1. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 1. 1. 0. 1. 1. 0. 0. 1. 0. 1. 1.\n",
            " 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1.\n",
            " 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1.\n",
            " 0. 1. 1. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1.]\n",
            "vote_pred [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 1. 1.\n",
            " 1. 1. 1. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 1. 1. 0. 1. 1. 0. 0. 0. 0. 1. 1.\n",
            " 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 0. 0.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1.\n",
            " 0. 1. 1. 1. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 1. 0. 1. 1. 1.]\n",
            "targetlist [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            "TP= 41 TN= 38 FN= 17 FP= 22\n",
            "TP+FP 63\n",
            "precision 0.6507936507936508\n",
            "recall 0.7068965517241379\n",
            "F1 0.6776859504132232\n",
            "acc 0.6694915254237288\n",
            "AUCp 0.6701149425287356\n",
            "AUC 0.7899425287356322\n",
            "\n",
            " The epoch is 10, average recall: 0.7069, average precision: 0.6508,average F1: 0.6777, average accuracy: 0.6695, average AUC: 0.7899\n",
            "Train Epoch: 11 [0/54 (0%)]\tTrain Loss: 0.002090\n",
            "Train Epoch: 11 [10/54 (19%)]\tTrain Loss: 0.001203\n",
            "Train Epoch: 11 [20/54 (37%)]\tTrain Loss: 0.000600\n",
            "Train Epoch: 11 [30/54 (56%)]\tTrain Loss: 0.026135\n",
            "Train Epoch: 11 [40/54 (74%)]\tTrain Loss: 0.001255\n",
            "Train Epoch: 11 [50/54 (93%)]\tTrain Loss: 0.000749\n",
            "\n",
            "Train set: Average loss: 0.0086, Accuracy: 415/425 (98%)\n",
            "\n",
            "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            "score [5.59205981e-03 4.97691840e-01 6.12102151e-02 1.85320638e-02\n",
            " 1.55457633e-03 1.14025339e-03 8.53770375e-01 8.48599449e-02\n",
            " 4.83846888e-02 3.66199277e-02 6.54473454e-02 2.64143590e-02\n",
            " 9.52223167e-02 5.60942106e-02 5.06510377e-01 1.21693370e-06\n",
            " 8.63904227e-03 3.41283470e-01 3.69997948e-01 4.74158116e-02\n",
            " 1.79832563e-01 8.02505374e-01 9.35078382e-01 1.00000000e+00\n",
            " 9.26534474e-01 9.77844596e-01 1.00000000e+00 3.99338305e-02\n",
            " 7.33043998e-03 2.77578793e-02 2.63764262e-01 2.44999185e-01\n",
            " 8.47119093e-02 6.68774810e-05 1.29903428e-05 1.44265685e-02\n",
            " 3.04541197e-02 1.78402379e-01 3.68208110e-01 2.65318364e-01\n",
            " 3.31034780e-01 7.29286313e-01 2.20241636e-01 1.67904329e-02\n",
            " 2.64945984e-01 1.86467227e-02 9.20958042e-01 6.33009136e-01\n",
            " 7.17287064e-01 4.83140469e-01 9.35076714e-01 2.86164554e-03\n",
            " 2.74862487e-06 1.02268634e-04 5.86232208e-02 2.50449034e-06\n",
            " 5.74202657e-01 2.09694662e-07 1.11468299e-03 5.26007323e-04\n",
            " 9.99673128e-01 9.99994040e-01 9.99607384e-01 9.99837875e-01\n",
            " 6.89498186e-01 9.80445445e-01 6.96964085e-01 1.00000000e+00\n",
            " 9.94709969e-01 6.69603467e-01 1.96713246e-02 8.16808119e-02\n",
            " 9.98716831e-01 4.24332440e-01 7.13306189e-01 9.70581889e-01\n",
            " 9.99832749e-01 9.99919772e-01 9.87529278e-01 9.96651709e-01\n",
            " 9.97125089e-01 9.94748056e-01 1.00000000e+00 7.44928598e-01\n",
            " 3.82457674e-01 9.25101519e-01 9.99034405e-01 9.92871225e-01\n",
            " 8.40819299e-01 9.99975085e-01 9.99275386e-01 2.34678015e-01\n",
            " 4.56303693e-02 9.96356010e-01 9.88595665e-01 9.82904792e-01\n",
            " 5.91156743e-02 7.69946218e-01 7.60147691e-01 9.98336911e-01\n",
            " 3.01977605e-01 2.22838655e-01 6.89488947e-01 7.49227405e-02\n",
            " 3.01283170e-02 6.14044480e-02 9.10024464e-01 2.61597365e-01\n",
            " 1.96709141e-01 2.80913562e-01 4.81366068e-01 2.62555987e-01\n",
            " 9.99418378e-01 9.99994636e-01 4.81991917e-01 9.80013728e-01\n",
            " 9.99929309e-01 9.99994636e-01]\n",
            "predict [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 1. 1.\n",
            " 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1.\n",
            " 1. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0.\n",
            " 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1.\n",
            " 0. 1. 1. 1. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 1. 0. 1. 1. 1.]\n",
            "Train Epoch: 12 [0/54 (0%)]\tTrain Loss: 0.010637\n",
            "Train Epoch: 12 [10/54 (19%)]\tTrain Loss: 0.006229\n",
            "Train Epoch: 12 [20/54 (37%)]\tTrain Loss: 0.015796\n",
            "Train Epoch: 12 [30/54 (56%)]\tTrain Loss: 0.012714\n",
            "Train Epoch: 12 [40/54 (74%)]\tTrain Loss: 0.001672\n",
            "Train Epoch: 12 [50/54 (93%)]\tTrain Loss: 0.038385\n",
            "\n",
            "Train set: Average loss: 0.0213, Accuracy: 403/425 (95%)\n",
            "\n",
            "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            "score [7.63773993e-02 4.55811322e-01 7.41694048e-02 1.69439882e-01\n",
            " 2.56362408e-02 3.80528122e-02 6.68301582e-01 2.75024772e-01\n",
            " 9.31958556e-02 1.81839213e-01 1.85626239e-01 1.00417890e-01\n",
            " 2.60672003e-01 4.40481864e-02 2.48483822e-01 2.55411083e-04\n",
            " 1.04115177e-02 3.72614205e-01 2.14276403e-01 7.09802583e-02\n",
            " 1.36988103e-01 3.96827549e-01 1.62515014e-01 9.98223007e-01\n",
            " 3.73119801e-01 4.73011702e-01 9.97950017e-01 7.72606432e-02\n",
            " 4.81600314e-02 5.77387027e-02 2.08349466e-01 2.90705085e-01\n",
            " 7.06743449e-02 1.82905555e-04 9.70517212e-05 3.28827202e-02\n",
            " 4.80843820e-02 2.93921769e-01 4.09604818e-01 4.28482234e-01\n",
            " 5.00058234e-01 5.41928232e-01 2.63938218e-01 9.54919159e-02\n",
            " 2.05355197e-01 1.36802986e-01 9.62950468e-01 8.78557265e-01\n",
            " 8.93095076e-01 7.80987561e-01 9.95596468e-01 1.65553428e-02\n",
            " 1.13160899e-02 3.92682804e-03 1.11852281e-01 1.12689380e-03\n",
            " 1.99372843e-01 3.32556374e-04 1.83645319e-02 2.61933077e-02\n",
            " 9.92842197e-01 9.96493399e-01 9.97248709e-01 9.97249067e-01\n",
            " 1.80450633e-01 4.73667949e-01 4.83470887e-01 9.99941707e-01\n",
            " 9.82082844e-01 8.56309593e-01 1.20668992e-01 3.42745304e-01\n",
            " 9.69184399e-01 2.49915138e-01 3.31882060e-01 8.79730821e-01\n",
            " 9.94623184e-01 9.95390296e-01 8.95722032e-01 9.58123624e-01\n",
            " 9.98033583e-01 9.86207724e-01 9.99771893e-01 8.68844271e-01\n",
            " 2.79812366e-01 9.98376012e-01 9.98705983e-01 9.84434187e-01\n",
            " 3.21603715e-01 9.17318225e-01 9.68751252e-01 6.14177883e-01\n",
            " 1.95176885e-01 9.38592672e-01 9.62819040e-01 7.11394131e-01\n",
            " 5.75472265e-02 8.09876084e-01 4.71109986e-01 9.95643854e-01\n",
            " 4.14013594e-01 3.97434801e-01 3.58494490e-01 3.44270438e-01\n",
            " 2.63169140e-01 4.15904611e-01 9.66068089e-01 1.84050143e-01\n",
            " 1.16549268e-01 1.50501385e-01 2.69240975e-01 2.79750079e-01\n",
            " 9.96891797e-01 9.82007563e-01 9.10262227e-01 9.18569684e-01\n",
            " 9.98857498e-01 9.82748210e-01]\n",
            "predict [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.\n",
            " 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 1. 1.\n",
            " 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 0. 0. 0. 1. 1. 1. 0. 0.\n",
            " 1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 0. 1. 1. 1. 0. 1. 1. 1.\n",
            " 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1.]\n",
            "Train Epoch: 13 [0/54 (0%)]\tTrain Loss: 0.003799\n",
            "Train Epoch: 13 [10/54 (19%)]\tTrain Loss: 0.000509\n",
            "Train Epoch: 13 [20/54 (37%)]\tTrain Loss: 0.053613\n",
            "Train Epoch: 13 [30/54 (56%)]\tTrain Loss: 0.005807\n",
            "Train Epoch: 13 [40/54 (74%)]\tTrain Loss: 0.006596\n",
            "Train Epoch: 13 [50/54 (93%)]\tTrain Loss: 0.003024\n",
            "\n",
            "Train set: Average loss: 0.0164, Accuracy: 413/425 (97%)\n",
            "\n",
            "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            "score [3.57958977e-03 1.67147696e-01 2.29664762e-02 3.90060223e-03\n",
            " 5.30420570e-04 2.16708868e-03 5.64663291e-01 4.31791246e-02\n",
            " 3.12508340e-03 1.15322217e-01 1.89790815e-01 2.92494111e-02\n",
            " 1.27024561e-01 8.20197910e-03 4.40898612e-02 5.83961373e-04\n",
            " 2.67685670e-03 7.10326154e-03 7.41345948e-03 4.63172793e-02\n",
            " 1.16705354e-02 2.71435436e-02 9.61795747e-02 9.93286908e-01\n",
            " 2.14371998e-02 3.01328003e-01 9.81017351e-01 4.60749418e-02\n",
            " 2.92085018e-02 9.24697667e-02 1.18621424e-01 3.76990139e-01\n",
            " 1.44720018e-01 5.83378226e-03 3.28121911e-04 1.94859616e-02\n",
            " 1.36921946e-02 7.48616643e-03 1.52867243e-01 2.11653113e-01\n",
            " 2.14053020e-01 1.56215608e-01 1.21461898e-01 1.34651586e-02\n",
            " 2.56062865e-01 1.53152533e-02 8.75728607e-01 6.47291839e-01\n",
            " 8.71453881e-01 6.71710908e-01 9.77427781e-01 3.82720173e-04\n",
            " 1.17041520e-03 1.20009982e-03 1.64206047e-02 1.23060436e-03\n",
            " 1.93544313e-01 4.68187704e-04 4.81532980e-03 2.58566369e-03\n",
            " 9.88859177e-01 9.92667854e-01 9.98697639e-01 9.98798013e-01\n",
            " 1.67111725e-01 4.75534886e-01 7.06947222e-02 9.99997854e-01\n",
            " 9.85874534e-01 9.22636271e-01 1.36676088e-01 2.39491254e-01\n",
            " 9.40803945e-01 3.78614128e-01 5.24803340e-01 9.92671251e-01\n",
            " 9.94134843e-01 9.97350693e-01 9.92336810e-01 9.86650229e-01\n",
            " 9.98949111e-01 9.94093359e-01 9.99050915e-01 4.54901755e-01\n",
            " 1.17870815e-01 9.92327988e-01 9.97376442e-01 9.82909977e-01\n",
            " 1.23058036e-01 7.00592458e-01 8.59966993e-01 2.00909987e-01\n",
            " 6.86435681e-03 9.16852713e-01 9.45539713e-01 3.95905644e-01\n",
            " 5.76279068e-04 7.00683117e-01 6.81971133e-01 9.79029298e-01\n",
            " 2.31003746e-01 7.14011550e-01 1.18287839e-01 1.59877688e-01\n",
            " 2.73193121e-01 3.70726377e-01 9.64703023e-01 2.66189575e-02\n",
            " 1.75829846e-02 7.97741339e-02 1.12591624e-01 1.31571084e-01\n",
            " 9.98274446e-01 9.04669523e-01 6.43686473e-01 9.68068719e-01\n",
            " 9.99734342e-01 9.82219696e-01]\n",
            "predict [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.\n",
            " 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1.\n",
            " 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 0. 0. 0. 1. 1. 1. 0. 0.\n",
            " 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 0. 1. 1. 0. 0. 1. 1. 0.\n",
            " 0. 1. 1. 1. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1.]\n",
            "Train Epoch: 14 [0/54 (0%)]\tTrain Loss: 0.004221\n",
            "Train Epoch: 14 [10/54 (19%)]\tTrain Loss: 0.004213\n",
            "Train Epoch: 14 [20/54 (37%)]\tTrain Loss: 0.000806\n",
            "Train Epoch: 14 [30/54 (56%)]\tTrain Loss: 0.000591\n",
            "Train Epoch: 14 [40/54 (74%)]\tTrain Loss: 0.004694\n",
            "Train Epoch: 14 [50/54 (93%)]\tTrain Loss: 0.004182\n",
            "\n",
            "Train set: Average loss: 0.0110, Accuracy: 417/425 (98%)\n",
            "\n",
            "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            "score [2.96354278e-06 1.57589391e-01 2.47261338e-02 5.75221889e-02\n",
            " 1.41547283e-03 8.15375461e-06 3.86251807e-01 1.06217273e-01\n",
            " 1.79785136e-02 1.83830395e-01 6.27491832e-01 1.13648556e-01\n",
            " 6.96492016e-01 4.12018932e-02 5.32569528e-01 6.87037536e-04\n",
            " 4.16000187e-03 2.35991895e-01 5.34373403e-01 4.64450866e-02\n",
            " 4.75108951e-01 8.37448180e-01 4.96513247e-01 9.99908447e-01\n",
            " 7.97125340e-01 8.11920047e-01 9.98514354e-01 7.73499906e-02\n",
            " 5.57335094e-02 1.54736713e-01 8.48560810e-01 6.10726476e-01\n",
            " 2.55887806e-01 1.27831081e-04 4.93073603e-04 2.44664133e-01\n",
            " 9.97215211e-02 4.22562182e-01 7.65327096e-01 9.38758910e-01\n",
            " 9.65862095e-01 9.16887879e-01 1.93399414e-01 2.66415477e-01\n",
            " 2.06421360e-01 4.23867255e-01 9.95519400e-01 9.78102148e-01\n",
            " 9.43281174e-01 8.72255564e-01 9.84602928e-01 4.14244179e-03\n",
            " 1.83008160e-04 8.34569510e-05 5.59779167e-01 4.73218824e-05\n",
            " 1.82276800e-01 5.79760490e-05 1.13456808e-04 3.47784996e-01\n",
            " 9.99972939e-01 9.99917269e-01 9.99937534e-01 9.99988556e-01\n",
            " 3.57683480e-01 9.58611965e-01 7.48112381e-01 1.00000000e+00\n",
            " 9.98065889e-01 9.89113569e-01 1.03567198e-01 4.07175362e-01\n",
            " 9.98235941e-01 7.64337182e-01 4.59184587e-01 9.99270737e-01\n",
            " 9.99442399e-01 9.99936461e-01 9.99978423e-01 9.99732554e-01\n",
            " 9.99917150e-01 9.99446452e-01 9.99997258e-01 9.98632491e-01\n",
            " 9.61473227e-01 9.99830365e-01 9.99651194e-01 9.90939736e-01\n",
            " 7.37443507e-01 9.97442365e-01 9.98375773e-01 5.69783449e-01\n",
            " 3.31713036e-02 9.90652204e-01 9.98688042e-01 5.33892989e-01\n",
            " 7.00075999e-02 9.77299929e-01 8.47885013e-01 9.95695829e-01\n",
            " 6.63647950e-01 8.63356650e-01 7.61765480e-01 8.63457397e-02\n",
            " 4.46776867e-01 6.08899713e-01 9.79978919e-01 1.22273915e-01\n",
            " 4.59765524e-01 2.41301693e-02 3.35049093e-01 1.40442252e-01\n",
            " 9.99880075e-01 9.77324665e-01 9.25338507e-01 9.99618053e-01\n",
            " 9.99999523e-01 9.99991775e-01]\n",
            "predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 0. 0. 0. 1. 0. 0. 1. 0. 1.\n",
            " 1. 1. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 0. 0. 0. 0. 1. 1.\n",
            " 1. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 0. 0.\n",
            " 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1.\n",
            " 0. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1.]\n",
            "Train Epoch: 15 [0/54 (0%)]\tTrain Loss: 0.000483\n",
            "Train Epoch: 15 [10/54 (19%)]\tTrain Loss: 0.004837\n",
            "Train Epoch: 15 [20/54 (37%)]\tTrain Loss: 0.005742\n",
            "Train Epoch: 15 [30/54 (56%)]\tTrain Loss: 0.002831\n",
            "Train Epoch: 15 [40/54 (74%)]\tTrain Loss: 0.000310\n",
            "Train Epoch: 15 [50/54 (93%)]\tTrain Loss: 0.000899\n",
            "\n",
            "Train set: Average loss: 0.0087, Accuracy: 419/425 (99%)\n",
            "\n",
            "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            "score [3.35734112e-05 4.14075911e-01 2.12939791e-02 7.28210388e-03\n",
            " 3.24035133e-03 9.94735747e-05 6.57854080e-01 9.30807590e-02\n",
            " 4.74465033e-03 5.47180176e-02 2.40558326e-01 3.17951813e-02\n",
            " 1.79957092e-01 8.38494860e-03 1.76078990e-01 1.57773189e-04\n",
            " 1.09695620e-03 2.65498925e-02 9.82083902e-02 9.21839010e-03\n",
            " 1.74170539e-01 6.77323163e-01 9.29037750e-01 9.99997377e-01\n",
            " 5.73515713e-01 7.84905255e-01 9.99993443e-01 1.86141416e-01\n",
            " 6.98990822e-02 8.62423331e-02 8.49646568e-01 9.97672856e-01\n",
            " 3.02779943e-01 2.99402192e-04 4.47632978e-04 4.62986790e-02\n",
            " 2.28657275e-02 4.58829194e-01 3.29865992e-01 2.85531282e-01\n",
            " 4.76866961e-01 7.00990260e-01 1.58673152e-01 2.12595031e-01\n",
            " 9.88535225e-01 3.90416570e-02 9.54155207e-01 9.66636956e-01\n",
            " 9.24889505e-01 4.66401845e-01 9.89170372e-01 5.39309112e-03\n",
            " 2.54274090e-03 1.46045187e-03 1.88303486e-01 1.67283299e-03\n",
            " 1.03719346e-01 2.73225218e-04 3.99599201e-04 1.38562754e-01\n",
            " 9.99999404e-01 9.99997258e-01 9.99999642e-01 9.99999285e-01\n",
            " 8.71632159e-01 9.99967456e-01 9.97799814e-01 1.00000000e+00\n",
            " 9.99973655e-01 9.90958452e-01 6.05362207e-02 3.50276262e-01\n",
            " 9.99999166e-01 8.93713355e-01 9.21138346e-01 9.99999881e-01\n",
            " 9.99999762e-01 9.99999881e-01 9.99999881e-01 9.99998569e-01\n",
            " 1.00000000e+00 1.00000000e+00 1.00000000e+00 9.95270669e-01\n",
            " 3.01477879e-01 9.99988675e-01 9.99998927e-01 9.99123871e-01\n",
            " 3.05843741e-01 9.99925494e-01 9.97743607e-01 1.04045138e-01\n",
            " 2.72054654e-02 9.98920679e-01 9.99895334e-01 4.99152988e-01\n",
            " 2.78700441e-02 9.15321767e-01 3.64872545e-01 9.27195132e-01\n",
            " 3.06889981e-01 8.20290804e-01 2.02361703e-01 1.18654137e-02\n",
            " 8.90113972e-03 4.68981788e-02 8.89735699e-01 2.50553973e-02\n",
            " 8.33499059e-03 6.48908736e-03 7.30479509e-02 3.19686830e-02\n",
            " 9.94216919e-01 9.99954462e-01 4.29624915e-01 9.99144316e-01\n",
            " 1.00000000e+00 9.99975681e-01]\n",
            "predict [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1.\n",
            " 1. 1. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 1. 1.\n",
            " 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 0. 1. 1. 0. 0. 1. 1. 0.\n",
            " 0. 1. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 1. 0. 1. 1. 1.]\n",
            "Train Epoch: 16 [0/54 (0%)]\tTrain Loss: 0.001512\n",
            "Train Epoch: 16 [10/54 (19%)]\tTrain Loss: 0.004699\n",
            "Train Epoch: 16 [20/54 (37%)]\tTrain Loss: 0.002222\n",
            "Train Epoch: 16 [30/54 (56%)]\tTrain Loss: 0.009275\n",
            "Train Epoch: 16 [40/54 (74%)]\tTrain Loss: 0.001147\n",
            "Train Epoch: 16 [50/54 (93%)]\tTrain Loss: 0.001882\n",
            "\n",
            "Train set: Average loss: 0.0082, Accuracy: 416/425 (98%)\n",
            "\n",
            "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            "score [1.90951528e-06 3.63703012e-01 1.66947655e-02 1.48740903e-01\n",
            " 4.05104889e-04 8.51272780e-05 4.74224210e-01 1.15102626e-01\n",
            " 6.59061456e-03 5.29126748e-02 6.77948952e-01 4.97198664e-02\n",
            " 4.11785871e-01 6.40319893e-03 4.70197529e-01 3.54831798e-07\n",
            " 1.70831065e-08 8.79107043e-02 1.07264653e-01 3.75300786e-03\n",
            " 9.14114043e-02 7.34778821e-01 2.63847768e-01 9.99822795e-01\n",
            " 3.96749526e-01 1.38906360e-01 9.99933362e-01 3.49996924e-01\n",
            " 3.88077423e-02 9.27105173e-02 9.62134361e-01 8.47792447e-01\n",
            " 3.36661823e-02 1.07512953e-07 7.69394433e-08 2.67001614e-02\n",
            " 1.71318967e-02 1.01721473e-01 9.89770770e-01 5.72879076e-01\n",
            " 8.61753881e-01 7.86955655e-01 1.06980048e-01 3.23520899e-02\n",
            " 4.96493727e-02 1.38278129e-02 9.14832234e-01 9.80419099e-01\n",
            " 7.99800694e-01 2.16518074e-01 9.63854074e-01 2.96546100e-03\n",
            " 2.66225223e-04 8.99807783e-05 2.50123084e-01 8.72297271e-04\n",
            " 1.89310700e-01 3.93004339e-05 9.06355199e-05 6.82799742e-02\n",
            " 9.99963284e-01 9.99974608e-01 9.99996662e-01 9.99968886e-01\n",
            " 9.25670862e-01 9.98398721e-01 9.93919730e-01 1.00000000e+00\n",
            " 9.99840975e-01 9.98390555e-01 2.40212362e-02 3.92306000e-01\n",
            " 9.99879122e-01 9.96403098e-01 9.54711616e-01 9.98811603e-01\n",
            " 9.99886394e-01 9.99989986e-01 9.99992609e-01 9.98286426e-01\n",
            " 9.99953151e-01 9.98184621e-01 9.99997020e-01 9.91821826e-01\n",
            " 5.38695216e-01 9.99139786e-01 9.99468982e-01 8.07618558e-01\n",
            " 4.52914655e-01 9.97772276e-01 9.99365151e-01 8.52283776e-01\n",
            " 3.09739299e-02 9.98930633e-01 9.99909878e-01 9.08301055e-01\n",
            " 5.89398406e-02 9.95311439e-01 4.08537567e-01 9.95226502e-01\n",
            " 2.20639527e-01 9.59463000e-01 9.95230615e-01 1.20571755e-01\n",
            " 6.21614158e-02 4.75984365e-01 9.88182366e-01 1.13352872e-01\n",
            " 9.78394449e-02 3.43415439e-02 3.81762117e-01 9.93839130e-02\n",
            " 9.95025694e-01 9.99072194e-01 6.89779103e-01 9.63466287e-01\n",
            " 9.99997616e-01 9.99994636e-01]\n",
            "predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1.\n",
            " 0. 0. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 0. 0. 0. 0. 1. 1.\n",
            " 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 0. 1. 1. 1.\n",
            " 0. 1. 0. 1. 0. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1.]\n",
            "Train Epoch: 17 [0/54 (0%)]\tTrain Loss: 0.026461\n",
            "Train Epoch: 17 [10/54 (19%)]\tTrain Loss: 0.002253\n",
            "Train Epoch: 17 [20/54 (37%)]\tTrain Loss: 0.008212\n",
            "Train Epoch: 17 [30/54 (56%)]\tTrain Loss: 0.000313\n",
            "Train Epoch: 17 [40/54 (74%)]\tTrain Loss: 0.000456\n",
            "Train Epoch: 17 [50/54 (93%)]\tTrain Loss: 0.010020\n",
            "\n",
            "Train set: Average loss: 0.0043, Accuracy: 419/425 (99%)\n",
            "\n",
            "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            "score [2.82733073e-08 5.75200796e-01 1.62167586e-02 5.99474624e-05\n",
            " 1.61731793e-06 3.59709156e-06 9.09864366e-01 3.54208425e-03\n",
            " 3.13831442e-05 2.44923867e-02 8.00189495e-01 2.20906306e-02\n",
            " 4.28512454e-01 7.83667667e-04 1.36952966e-01 8.86789593e-08\n",
            " 2.33226979e-11 2.93486664e-04 6.36622488e-01 2.11895276e-02\n",
            " 8.81479084e-01 6.88795865e-01 6.76812470e-01 9.99683499e-01\n",
            " 3.94074917e-01 6.39669001e-01 9.99838710e-01 1.19784147e-01\n",
            " 1.19654443e-02 4.75967936e-02 8.73045146e-01 9.59076345e-01\n",
            " 2.59943247e-01 3.31061706e-10 9.71683711e-09 4.72936826e-03\n",
            " 4.34494345e-03 5.58459386e-02 3.64158303e-01 3.54468256e-01\n",
            " 7.21643627e-01 7.56775618e-01 5.05321436e-02 2.16435596e-01\n",
            " 2.40287811e-01 5.72413743e-01 9.97886837e-01 9.99378562e-01\n",
            " 9.87502217e-01 1.54024944e-01 9.99296069e-01 6.70728798e-04\n",
            " 3.62483661e-07 8.52720586e-06 6.19269088e-02 1.84169330e-05\n",
            " 2.78372280e-02 6.45450370e-07 9.33125932e-07 4.58908640e-03\n",
            " 9.99999523e-01 9.99999881e-01 1.00000000e+00 9.99999881e-01\n",
            " 9.58633423e-01 9.95621741e-01 9.23856854e-01 1.00000000e+00\n",
            " 9.99954343e-01 9.99533892e-01 1.62413847e-02 4.56711471e-01\n",
            " 9.99925137e-01 9.79657948e-01 9.86186028e-01 9.99965549e-01\n",
            " 9.99986291e-01 9.99996543e-01 1.00000000e+00 9.99835730e-01\n",
            " 9.99998450e-01 9.99990702e-01 9.99999881e-01 9.98095930e-01\n",
            " 9.21707451e-01 9.99995589e-01 9.99936223e-01 9.64290380e-01\n",
            " 3.51741537e-02 9.66782808e-01 9.95409191e-01 3.91931355e-01\n",
            " 1.75799426e-06 9.95210707e-01 9.99951243e-01 8.63431811e-01\n",
            " 4.32165852e-03 9.54860628e-01 9.76279199e-01 9.99700904e-01\n",
            " 2.38260090e-01 9.35760915e-01 5.73204637e-01 2.18402110e-02\n",
            " 8.89917929e-03 9.41944197e-02 9.99597371e-01 2.31300741e-01\n",
            " 2.73110643e-02 4.11680281e-01 7.11608171e-01 1.10635921e-01\n",
            " 9.99810636e-01 9.99851346e-01 5.32462776e-01 9.92170155e-01\n",
            " 9.99999642e-01 9.99999404e-01]\n",
            "predict [0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 1. 1.\n",
            " 0. 1. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 1. 1. 1.\n",
            " 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 0. 0. 1. 1. 1.\n",
            " 0. 1. 1. 1. 0. 1. 1. 0. 0. 0. 1. 0. 0. 0. 1. 0. 1. 1. 1. 1. 1. 1.]\n",
            "Train Epoch: 18 [0/54 (0%)]\tTrain Loss: 0.001079\n",
            "Train Epoch: 18 [10/54 (19%)]\tTrain Loss: 0.006235\n",
            "Train Epoch: 18 [20/54 (37%)]\tTrain Loss: 0.000550\n",
            "Train Epoch: 18 [30/54 (56%)]\tTrain Loss: 0.002624\n",
            "Train Epoch: 18 [40/54 (74%)]\tTrain Loss: 0.000087\n",
            "Train Epoch: 18 [50/54 (93%)]\tTrain Loss: 0.055054\n",
            "\n",
            "Train set: Average loss: 0.0072, Accuracy: 418/425 (98%)\n",
            "\n",
            "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            "score [2.82242763e-06 9.97081339e-01 7.97598839e-01 4.35456604e-04\n",
            " 3.74509873e-05 1.20399227e-05 9.97950137e-01 1.19230337e-03\n",
            " 4.85788405e-05 3.30385007e-02 4.75530833e-01 6.90237992e-03\n",
            " 1.01943590e-01 1.10049993e-02 2.88491040e-01 2.59982312e-06\n",
            " 1.43593297e-05 8.22547916e-03 2.12713748e-01 2.26254255e-01\n",
            " 8.52179348e-01 9.75323141e-01 9.14287388e-01 9.99735773e-01\n",
            " 8.98315370e-01 9.83948290e-01 9.99979019e-01 3.08759689e-01\n",
            " 1.52737806e-02 4.24675383e-02 9.64817405e-01 9.94527638e-01\n",
            " 8.86117160e-01 1.88871275e-06 1.53408166e-06 7.77156651e-03\n",
            " 2.94705201e-03 1.69198513e-02 6.81078613e-01 2.35892460e-01\n",
            " 4.61917400e-01 8.07940602e-01 6.27216976e-03 1.52798325e-01\n",
            " 2.24485025e-01 3.80161911e-01 9.70032454e-01 9.89052951e-01\n",
            " 8.29754174e-01 2.45943919e-01 9.92200077e-01 4.84979217e-04\n",
            " 7.70416500e-06 4.91090905e-05 1.14056398e-03 3.23962944e-04\n",
            " 2.22152323e-02 4.49039089e-06 2.43505801e-05 1.42174901e-03\n",
            " 9.99931931e-01 9.99992013e-01 9.99998569e-01 9.99997139e-01\n",
            " 9.60220814e-01 9.81260061e-01 9.87515926e-01 1.00000000e+00\n",
            " 9.99737680e-01 9.99363482e-01 6.42458916e-01 8.14485610e-01\n",
            " 9.99950886e-01 8.50166082e-01 9.92815375e-01 9.99669194e-01\n",
            " 9.99987006e-01 9.99997973e-01 9.99999523e-01 9.99783814e-01\n",
            " 9.99975562e-01 9.99937892e-01 1.00000000e+00 9.98590529e-01\n",
            " 4.16857958e-01 9.99947429e-01 9.99982595e-01 9.86129105e-01\n",
            " 2.95872837e-01 9.98790205e-01 9.99574840e-01 1.47487611e-01\n",
            " 7.76454690e-04 9.95834470e-01 9.99992847e-01 8.56703341e-01\n",
            " 4.36825538e-03 9.58062351e-01 9.95013773e-01 9.99988079e-01\n",
            " 6.98439538e-01 9.97561216e-01 4.08754766e-01 5.56947775e-02\n",
            " 5.93672432e-02 2.95612097e-01 9.99884844e-01 3.17422710e-02\n",
            " 7.54661253e-03 3.93373311e-01 1.91385373e-01 4.73770574e-02\n",
            " 9.99543488e-01 9.99991417e-01 9.44046676e-01 9.99904513e-01\n",
            " 9.99973416e-01 1.00000000e+00]\n",
            "predict [0. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1.\n",
            " 1. 1. 1. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 1. 1.\n",
            " 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 0. 1. 1. 0. 0. 1. 1. 1.\n",
            " 0. 1. 1. 1. 1. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1.]\n",
            "Train Epoch: 19 [0/54 (0%)]\tTrain Loss: 0.000300\n",
            "Train Epoch: 19 [10/54 (19%)]\tTrain Loss: 0.000980\n",
            "Train Epoch: 19 [20/54 (37%)]\tTrain Loss: 0.000101\n",
            "Train Epoch: 19 [30/54 (56%)]\tTrain Loss: 0.004748\n",
            "Train Epoch: 19 [40/54 (74%)]\tTrain Loss: 0.056659\n",
            "Train Epoch: 19 [50/54 (93%)]\tTrain Loss: 0.000455\n",
            "\n",
            "Train set: Average loss: 0.0117, Accuracy: 416/425 (98%)\n",
            "\n",
            "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            "score [1.17956915e-06 2.75265634e-01 2.37147167e-01 2.41719349e-03\n",
            " 4.26912535e-04 1.68058814e-05 7.06659913e-01 2.83477711e-03\n",
            " 7.54247827e-04 1.94185413e-02 2.60808885e-01 1.67855844e-02\n",
            " 5.17964140e-02 6.51732832e-03 2.58489586e-02 7.67614347e-06\n",
            " 3.53986943e-05 6.15993235e-03 7.84231201e-02 1.02008665e-02\n",
            " 4.57011849e-01 1.48299277e-01 1.21104136e-01 9.99990225e-01\n",
            " 1.06053859e-01 2.58607924e-01 9.99968886e-01 5.77357411e-03\n",
            " 4.10697330e-03 2.04934068e-02 9.70585458e-03 8.66322741e-02\n",
            " 1.83800012e-02 8.37522163e-13 4.23318103e-10 1.98487798e-03\n",
            " 1.06044742e-03 2.25771703e-02 6.42814115e-02 1.53315291e-01\n",
            " 4.42676932e-01 2.41927534e-01 3.65799898e-03 1.24031138e-02\n",
            " 9.32604820e-03 2.51396606e-03 2.45325997e-01 7.64726102e-01\n",
            " 6.42344505e-02 7.99711570e-02 5.16134143e-01 1.93223314e-04\n",
            " 3.47808582e-09 2.24747382e-07 6.63836347e-03 2.49077516e-06\n",
            " 6.31730957e-03 6.69055851e-08 2.74925952e-07 5.77372825e-03\n",
            " 9.99826729e-01 9.98211980e-01 9.96924818e-01 9.99069035e-01\n",
            " 2.10534126e-01 8.91498148e-01 2.60375559e-01 1.00000000e+00\n",
            " 9.99993682e-01 8.24588358e-01 1.94845032e-02 3.44366170e-02\n",
            " 9.59312022e-01 1.81340054e-01 6.67147413e-02 9.82971370e-01\n",
            " 9.97570097e-01 9.99503374e-01 9.99999762e-01 9.84895170e-01\n",
            " 9.99792755e-01 9.98582482e-01 1.00000000e+00 8.78922343e-01\n",
            " 2.05968052e-01 9.95652556e-01 9.99619246e-01 3.66686732e-01\n",
            " 2.89942384e-01 9.99972343e-01 9.99783218e-01 2.69995719e-01\n",
            " 8.68236530e-04 9.99979377e-01 1.00000000e+00 9.98529315e-01\n",
            " 3.40169668e-02 8.59735012e-01 8.44772160e-01 9.78325963e-01\n",
            " 4.18613851e-01 2.15007037e-01 7.91084766e-02 9.14789084e-03\n",
            " 4.59969342e-02 9.80633080e-01 9.99992609e-01 1.52982920e-02\n",
            " 1.27518887e-03 3.47761549e-02 7.96947852e-02 2.65320968e-02\n",
            " 9.96484637e-01 9.99999642e-01 3.95550728e-01 7.09075570e-01\n",
            " 1.00000000e+00 1.00000000e+00]\n",
            "predict [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.\n",
            " 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.\n",
            " 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 0. 1. 0. 1. 1. 1. 0. 0.\n",
            " 1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 0. 0. 1. 1. 0. 0. 1. 1. 1.\n",
            " 0. 1. 1. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 1. 1. 0. 1. 1. 1.]\n",
            "Train Epoch: 20 [0/54 (0%)]\tTrain Loss: 0.001798\n",
            "Train Epoch: 20 [10/54 (19%)]\tTrain Loss: 0.014073\n",
            "Train Epoch: 20 [20/54 (37%)]\tTrain Loss: 0.000338\n",
            "Train Epoch: 20 [30/54 (56%)]\tTrain Loss: 0.002664\n",
            "Train Epoch: 20 [40/54 (74%)]\tTrain Loss: 0.001372\n",
            "Train Epoch: 20 [50/54 (93%)]\tTrain Loss: 0.000679\n",
            "\n",
            "Train set: Average loss: 0.0043, Accuracy: 420/425 (99%)\n",
            "\n",
            "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            "score [2.17747993e-06 7.81146884e-02 2.39696410e-02 8.42574809e-04\n",
            " 2.82961671e-04 5.42700946e-06 9.21864212e-02 6.43106643e-03\n",
            " 1.90152612e-04 1.40261613e-02 4.93596405e-01 1.10460846e-02\n",
            " 2.18264684e-01 7.85379298e-03 3.11559260e-01 2.12081636e-06\n",
            " 1.03263300e-08 6.75818603e-03 3.06782369e-02 6.62715966e-03\n",
            " 2.71402627e-01 8.04114342e-02 2.82684416e-01 9.98419046e-01\n",
            " 4.57964391e-02 4.90125239e-01 9.93199885e-01 2.62530358e-03\n",
            " 3.50358663e-03 2.67126299e-02 2.54420757e-01 7.15701640e-01\n",
            " 1.60889164e-01 2.34070922e-12 7.15604550e-13 2.05206350e-04\n",
            " 4.91732408e-06 1.52620878e-02 6.73486292e-01 3.48106593e-01\n",
            " 7.92958260e-01 6.52471006e-01 1.48804567e-03 1.75097715e-02\n",
            " 1.01829339e-02 2.86717210e-02 7.34792888e-01 9.43966687e-01\n",
            " 6.31858885e-01 9.52291116e-02 9.17049170e-01 1.71365869e-07\n",
            " 6.98145155e-14 5.25914000e-11 1.04085411e-04 4.76911046e-05\n",
            " 1.94853827e-04 9.73334835e-08 2.19902915e-10 4.17787302e-03\n",
            " 9.96333361e-01 9.95627761e-01 9.96965945e-01 9.97756660e-01\n",
            " 7.31964588e-01 9.53523219e-01 7.16423333e-01 9.99557316e-01\n",
            " 9.97934818e-01 9.92798030e-01 3.20565224e-01 6.93900168e-01\n",
            " 9.98331726e-01 9.38199937e-01 3.97085160e-01 9.80802894e-01\n",
            " 9.97220278e-01 9.98261988e-01 9.98935163e-01 9.87337410e-01\n",
            " 9.97218847e-01 9.95599151e-01 9.99983072e-01 9.59879994e-01\n",
            " 4.30735290e-01 9.97817278e-01 9.82442975e-01 6.77772760e-01\n",
            " 7.89956272e-01 9.88045394e-01 9.97694790e-01 3.27572048e-01\n",
            " 1.19158030e-04 9.94624019e-01 9.99491692e-01 9.86651897e-01\n",
            " 2.68526166e-03 8.73500526e-01 8.03712130e-01 9.83559608e-01\n",
            " 6.49018109e-01 9.42239106e-01 6.72921598e-01 1.33083640e-02\n",
            " 1.23798236e-01 9.59348679e-01 9.99559581e-01 2.09009331e-02\n",
            " 2.38453926e-04 5.01148887e-02 1.76271692e-01 5.36529310e-02\n",
            " 9.78854179e-01 9.98317719e-01 9.68189776e-01 7.94043839e-01\n",
            " 9.99925852e-01 9.99999881e-01]\n",
            "predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.\n",
            " 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 0. 1. 1.\n",
            " 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1.\n",
            " 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1.\n",
            " 0. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1.]\n",
            "vote_pred [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1.\n",
            " 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1.\n",
            " 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 0. 1. 1. 0. 0. 1. 1. 1.\n",
            " 0. 1. 1. 1. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1.]\n",
            "targetlist [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            "TP= 41 TN= 50 FN= 17 FP= 10\n",
            "TP+FP 51\n",
            "precision 0.803921568627451\n",
            "recall 0.7068965517241379\n",
            "F1 0.7522935779816514\n",
            "acc 0.7711864406779662\n",
            "AUCp 0.7701149425287357\n",
            "AUC 0.8413793103448276\n",
            "\n",
            " The epoch is 20, average recall: 0.7069, average precision: 0.8039,average F1: 0.7523, average accuracy: 0.7712, average AUC: 0.8414\n",
            "Train Epoch: 21 [0/54 (0%)]\tTrain Loss: 0.000296\n",
            "Train Epoch: 21 [10/54 (19%)]\tTrain Loss: 0.023967\n",
            "Train Epoch: 21 [20/54 (37%)]\tTrain Loss: 0.000824\n",
            "Train Epoch: 21 [30/54 (56%)]\tTrain Loss: 0.004672\n",
            "Train Epoch: 21 [40/54 (74%)]\tTrain Loss: 0.000292\n",
            "Train Epoch: 21 [50/54 (93%)]\tTrain Loss: 0.000059\n",
            "\n",
            "Train set: Average loss: 0.0062, Accuracy: 418/425 (98%)\n",
            "\n",
            "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            "score [2.48747920e-08 1.24577805e-02 2.49808244e-02 2.69631446e-08\n",
            " 7.31819512e-08 5.99369571e-07 1.31366894e-01 1.98402384e-04\n",
            " 9.23374273e-07 2.42554545e-02 3.55569184e-01 1.30404811e-02\n",
            " 1.63251728e-01 2.04104672e-05 2.76556984e-03 1.36221395e-10\n",
            " 3.19879155e-15 1.66125332e-07 1.04858941e-06 2.09131158e-05\n",
            " 1.34414918e-11 5.27009543e-04 1.42897489e-02 9.97035980e-01\n",
            " 5.51001503e-05 2.48781983e-02 9.92719531e-01 6.01434351e-07\n",
            " 7.43608052e-06 4.45620710e-04 2.45763622e-05 1.69515442e-02\n",
            " 2.19714525e-03 4.19066122e-13 1.11215532e-16 6.72562805e-08\n",
            " 2.63604277e-10 7.98633937e-06 2.04598960e-02 5.46358973e-02\n",
            " 1.65134966e-01 5.47103025e-03 3.03724363e-07 3.90786299e-05\n",
            " 9.30571742e-03 1.21293298e-04 8.09613764e-01 8.41931939e-01\n",
            " 5.71879148e-01 3.17446068e-02 8.85409713e-01 1.62398266e-07\n",
            " 1.10876924e-11 1.16043867e-11 1.24172177e-04 9.15722921e-05\n",
            " 3.21584684e-03 4.69752131e-08 8.91182874e-12 2.02703191e-07\n",
            " 9.93166387e-01 9.94181812e-01 9.88892853e-01 9.98943865e-01\n",
            " 1.88398823e-01 8.07893574e-01 1.80326730e-01 9.99694109e-01\n",
            " 9.99860287e-01 9.77006495e-01 1.87059236e-03 1.13609910e-01\n",
            " 9.68698263e-01 1.69154838e-01 3.31804827e-02 8.20924044e-01\n",
            " 9.99705732e-01 9.99705017e-01 9.98757958e-01 9.72549617e-01\n",
            " 9.99427974e-01 9.92172837e-01 9.99970675e-01 9.21393931e-03\n",
            " 8.13759957e-03 9.94968474e-01 9.92429674e-01 4.48493838e-01\n",
            " 3.08736004e-02 9.91808474e-01 9.87326026e-01 1.31357208e-01\n",
            " 1.27825750e-09 9.98049974e-01 9.99999762e-01 9.95189190e-01\n",
            " 4.18300537e-04 8.16224635e-01 5.53109884e-01 5.09575069e-01\n",
            " 5.00528574e-01 3.35033357e-01 6.06639683e-02 7.81128241e-04\n",
            " 6.42368421e-02 9.89521861e-01 9.99984026e-01 7.46088009e-03\n",
            " 7.00640594e-05 4.56013195e-02 2.71411240e-01 1.09483369e-01\n",
            " 9.99786913e-01 9.77570415e-01 1.12022243e-01 5.08886063e-03\n",
            " 1.00000000e+00 1.00000000e+00]\n",
            "predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.\n",
            " 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1.\n",
            " 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 0. 1. 0. 1. 1. 1. 0. 0.\n",
            " 1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 0. 0. 1. 1. 0. 0. 1. 1. 1.\n",
            " 0. 1. 1. 1. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 1.]\n",
            "Train Epoch: 22 [0/54 (0%)]\tTrain Loss: 0.000158\n",
            "Train Epoch: 22 [10/54 (19%)]\tTrain Loss: 0.001234\n",
            "Train Epoch: 22 [20/54 (37%)]\tTrain Loss: 0.000557\n",
            "Train Epoch: 22 [30/54 (56%)]\tTrain Loss: 0.020480\n",
            "Train Epoch: 22 [40/54 (74%)]\tTrain Loss: 0.000684\n",
            "Train Epoch: 22 [50/54 (93%)]\tTrain Loss: 0.001372\n",
            "\n",
            "Train set: Average loss: 0.0041, Accuracy: 422/425 (99%)\n",
            "\n",
            "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            "score [4.19888067e-07 3.13266856e-03 8.30620527e-03 7.40375777e-04\n",
            " 2.99232361e-05 9.54861480e-06 4.45319302e-02 4.70994134e-03\n",
            " 5.87372706e-05 4.31725085e-02 4.38934475e-01 2.59242933e-02\n",
            " 1.88264906e-01 2.36503454e-03 2.80536991e-02 5.03190289e-09\n",
            " 1.09134257e-10 1.24244660e-04 1.11331856e-05 1.09967958e-04\n",
            " 2.37936559e-09 3.61683662e-03 2.74761543e-02 9.99977350e-01\n",
            " 2.58505880e-03 1.92126736e-01 9.99271095e-01 1.07762789e-04\n",
            " 4.57679416e-04 3.10588721e-03 4.98172641e-03 5.04984260e-01\n",
            " 5.60639463e-02 2.12111981e-10 2.33983666e-10 2.00111128e-04\n",
            " 1.84593428e-05 1.40981283e-02 1.16918266e-01 2.29740113e-01\n",
            " 5.43786526e-01 7.61234835e-02 1.84348819e-03 1.32304905e-02\n",
            " 2.37573590e-02 4.13876325e-02 9.92343247e-01 9.80775476e-01\n",
            " 8.82482708e-01 4.97333631e-02 9.64228868e-01 3.92302081e-06\n",
            " 1.18947222e-11 3.00599301e-09 5.49335673e-05 5.13173698e-04\n",
            " 8.50472134e-04 2.23513197e-07 1.03452850e-10 4.21292032e-04\n",
            " 9.99086380e-01 9.98364031e-01 9.99144197e-01 9.99984980e-01\n",
            " 1.57717213e-01 9.98497486e-01 4.19026047e-01 1.00000000e+00\n",
            " 9.99999881e-01 9.99081254e-01 6.77253678e-03 1.49773836e-01\n",
            " 9.93889213e-01 2.27240726e-01 1.40875980e-01 9.97205198e-01\n",
            " 9.99993801e-01 9.99971509e-01 1.00000000e+00 9.99999523e-01\n",
            " 1.00000000e+00 9.99991536e-01 1.00000000e+00 1.09333970e-01\n",
            " 3.39402296e-02 9.98814702e-01 9.99887109e-01 8.87826383e-01\n",
            " 6.84767682e-03 9.99986053e-01 9.62820709e-01 1.32363185e-01\n",
            " 4.54131111e-07 9.98232603e-01 9.99999762e-01 9.85869408e-01\n",
            " 1.40111998e-03 6.36636794e-01 1.02189645e-01 3.39311898e-01\n",
            " 6.34185374e-02 4.53272849e-01 3.49949449e-02 9.92629677e-04\n",
            " 2.23481469e-02 7.08852410e-01 9.99967217e-01 1.02664176e-02\n",
            " 7.90744525e-05 7.39224441e-03 1.07746534e-01 5.96166775e-02\n",
            " 9.90151942e-01 9.99639392e-01 6.32270515e-01 5.25650196e-03\n",
            " 1.00000000e+00 1.00000000e+00]\n",
            "predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.\n",
            " 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 1.\n",
            " 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 0. 1. 0. 1. 1. 1. 0. 0.\n",
            " 1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 0. 1. 1. 0. 0. 1. 1. 1.\n",
            " 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 1. 1. 1. 0. 1. 1.]\n",
            "Train Epoch: 23 [0/54 (0%)]\tTrain Loss: 0.000247\n",
            "Train Epoch: 23 [10/54 (19%)]\tTrain Loss: 0.000135\n",
            "Train Epoch: 23 [20/54 (37%)]\tTrain Loss: 0.001160\n",
            "Train Epoch: 23 [30/54 (56%)]\tTrain Loss: 0.004113\n",
            "Train Epoch: 23 [40/54 (74%)]\tTrain Loss: 0.000073\n",
            "Train Epoch: 23 [50/54 (93%)]\tTrain Loss: 0.000229\n",
            "\n",
            "Train set: Average loss: 0.0110, Accuracy: 414/425 (97%)\n",
            "\n",
            "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            "score [2.86179238e-05 1.09108659e-02 9.05314181e-03 1.01527828e-03\n",
            " 2.26543634e-04 7.14190654e-04 2.26222761e-02 8.04331992e-03\n",
            " 3.07355134e-04 1.53016165e-01 7.57471323e-01 6.05547801e-02\n",
            " 7.51924634e-01 6.69025555e-02 4.61658150e-01 1.83117645e-05\n",
            " 1.13700482e-03 7.71823153e-02 1.01794954e-03 1.01716234e-03\n",
            " 8.09986889e-03 1.81127921e-01 3.45089287e-01 9.99421835e-01\n",
            " 6.09431446e-01 7.33974159e-01 9.97812510e-01 8.55950825e-03\n",
            " 1.66378240e-03 3.27975005e-02 2.64684349e-01 9.90035355e-01\n",
            " 6.72744751e-01 6.55748727e-06 5.36273583e-04 2.15620715e-02\n",
            " 3.52214731e-04 4.07936871e-02 6.11803055e-01 4.76901531e-01\n",
            " 8.07805002e-01 7.80264795e-01 2.25453712e-02 4.62775491e-02\n",
            " 4.22900021e-01 3.19982618e-01 9.93667185e-01 9.97916758e-01\n",
            " 9.91317630e-01 7.40759254e-01 9.99459803e-01 2.56190542e-04\n",
            " 3.69426829e-07 2.50317898e-05 8.35873594e-04 2.03065734e-04\n",
            " 1.73733041e-01 4.69799261e-06 1.43788839e-06 5.42219877e-02\n",
            " 9.99953866e-01 9.99950767e-01 9.99967337e-01 9.99994874e-01\n",
            " 3.34441453e-01 9.91150022e-01 8.61447632e-01 9.99979973e-01\n",
            " 9.98125851e-01 9.96824741e-01 6.58546761e-02 5.98748624e-01\n",
            " 9.99841332e-01 9.08321977e-01 8.39401722e-01 9.98621345e-01\n",
            " 9.99898314e-01 9.99843240e-01 9.99935746e-01 9.99768674e-01\n",
            " 9.99969602e-01 9.92176116e-01 9.99999881e-01 9.93285716e-01\n",
            " 4.02107425e-02 9.99647021e-01 9.99864697e-01 9.98298824e-01\n",
            " 6.59129322e-01 9.98577476e-01 9.73009050e-01 1.03338465e-01\n",
            " 2.36060051e-03 9.99075770e-01 9.99804795e-01 9.93592203e-01\n",
            " 9.17122662e-02 5.52113235e-01 5.62171638e-01 9.48810816e-01\n",
            " 3.36288214e-01 8.64843369e-01 9.87658441e-01 6.06724173e-02\n",
            " 1.40937001e-01 4.07226123e-02 9.99405265e-01 5.79439290e-02\n",
            " 1.58475386e-03 5.97919486e-02 1.13215461e-01 3.41512151e-02\n",
            " 9.80652809e-01 9.96557355e-01 9.62200582e-01 4.21201736e-02\n",
            " 9.99693274e-01 9.99954343e-01]\n",
            "predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.\n",
            " 1. 1. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 0. 1. 1.\n",
            " 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 0. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1.\n",
            " 0. 1. 1. 1. 0. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 1. 1. 0. 1. 1.]\n",
            "Train Epoch: 24 [0/54 (0%)]\tTrain Loss: 0.000423\n",
            "Train Epoch: 24 [10/54 (19%)]\tTrain Loss: 0.001619\n",
            "Train Epoch: 24 [20/54 (37%)]\tTrain Loss: 0.001115\n",
            "Train Epoch: 24 [30/54 (56%)]\tTrain Loss: 0.004065\n",
            "Train Epoch: 24 [40/54 (74%)]\tTrain Loss: 0.002915\n",
            "Train Epoch: 24 [50/54 (93%)]\tTrain Loss: 0.014154\n",
            "\n",
            "Train set: Average loss: 0.0249, Accuracy: 406/425 (96%)\n",
            "\n",
            "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            "score [0.02843867 0.38997269 0.16573919 0.04487309 0.03183214 0.03968896\n",
            " 0.55690765 0.03043318 0.07426267 0.11357773 0.32713127 0.04097692\n",
            " 0.12140368 0.40409058 0.35326332 0.04082568 0.05172084 0.18722598\n",
            " 0.51383358 0.03071662 0.81769764 0.68956113 0.90900171 0.98275405\n",
            " 0.60960937 0.86625385 0.95180613 0.37497911 0.05341542 0.07986717\n",
            " 0.80409342 0.85821772 0.36489484 0.0422206  0.037997   0.05999549\n",
            " 0.0552966  0.10417078 0.80996245 0.86413985 0.96433628 0.8443048\n",
            " 0.07641175 0.0789966  0.05374897 0.91368097 0.7724905  0.878865\n",
            " 0.97411716 0.97639799 0.99916697 0.03246947 0.02143883 0.02001413\n",
            " 0.41088238 0.09360182 0.12027162 0.01418222 0.02698005 0.04085996\n",
            " 0.99995756 0.99998796 0.99995208 0.99999201 0.82501441 0.96807265\n",
            " 0.91413963 0.99996483 0.99818075 0.99111652 0.97306764 0.88181621\n",
            " 0.99956971 0.9973079  0.95305401 0.99256575 0.99895132 0.99989331\n",
            " 0.9999944  0.98978561 0.99950647 0.99999046 1.         0.99133825\n",
            " 0.55455577 0.9987011  0.9926905  0.794599   0.78751057 0.99968195\n",
            " 0.99803525 0.19479856 0.16186266 0.98715419 0.99258536 0.99206614\n",
            " 0.35446408 0.9481411  0.94078296 0.99863607 0.35178223 0.93379641\n",
            " 0.979891   0.6999507  0.64737684 0.89714557 0.99605209 0.37637886\n",
            " 0.0763453  0.65576327 0.29595599 0.22083136 0.97982222 0.99455124\n",
            " 0.93041164 0.85223508 0.99989903 0.99963605]\n",
            "predict [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 1. 1.\n",
            " 1. 1. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 0. 0. 0. 1. 1. 1.\n",
            " 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1.\n",
            " 0. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 0. 0. 1. 0. 0. 1. 1. 1. 1. 1. 1.]\n",
            "Train Epoch: 25 [0/54 (0%)]\tTrain Loss: 0.003515\n",
            "Train Epoch: 25 [10/54 (19%)]\tTrain Loss: 0.011177\n",
            "Train Epoch: 25 [20/54 (37%)]\tTrain Loss: 0.002155\n",
            "Train Epoch: 25 [30/54 (56%)]\tTrain Loss: 0.001133\n",
            "Train Epoch: 25 [40/54 (74%)]\tTrain Loss: 0.001069\n",
            "Train Epoch: 25 [50/54 (93%)]\tTrain Loss: 0.000137\n",
            "\n",
            "Train set: Average loss: 0.0052, Accuracy: 422/425 (99%)\n",
            "\n",
            "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            "score [1.50397525e-03 9.09932554e-01 1.55458286e-01 1.22188795e-02\n",
            " 1.63356727e-03 7.69307604e-03 9.49306309e-01 2.97235069e-03\n",
            " 1.16219021e-01 5.39878756e-02 7.23915845e-02 8.56057648e-03\n",
            " 2.82160360e-02 7.17005059e-02 1.22372255e-01 1.07354566e-03\n",
            " 6.20688125e-03 1.04868539e-01 3.06624383e-01 4.18297900e-03\n",
            " 5.25964320e-01 8.79643083e-01 9.48196888e-01 9.98974562e-01\n",
            " 6.06632173e-01 9.67419982e-01 9.89323556e-01 7.90743008e-02\n",
            " 5.03684953e-03 2.12471336e-02 4.48951453e-01 8.61606598e-01\n",
            " 5.30971408e-01 1.88320386e-03 1.85101270e-03 6.51606033e-03\n",
            " 5.66134695e-03 7.65441477e-01 9.12378132e-01 7.41559386e-01\n",
            " 9.32792962e-01 2.72431612e-01 1.26356944e-01 5.35608120e-02\n",
            " 3.22423518e-01 1.55535698e-01 7.08082139e-01 9.08629775e-01\n",
            " 9.35555637e-01 9.35626388e-01 9.92722690e-01 3.41846258e-03\n",
            " 5.26461168e-04 1.59772672e-03 6.31023478e-03 3.57448286e-03\n",
            " 5.70948087e-02 4.29540582e-04 1.64309726e-03 2.66566081e-03\n",
            " 9.99746859e-01 9.99731243e-01 9.99523997e-01 9.99796808e-01\n",
            " 9.72978055e-01 9.97380912e-01 9.95831072e-01 9.99906659e-01\n",
            " 9.99953508e-01 9.75367606e-01 6.68289483e-01 4.13169891e-01\n",
            " 9.99968886e-01 9.92098093e-01 9.85886991e-01 9.99036074e-01\n",
            " 9.99994993e-01 9.99982476e-01 9.99995947e-01 9.99346793e-01\n",
            " 9.99985576e-01 1.00000000e+00 1.00000000e+00 9.95244563e-01\n",
            " 1.79016620e-01 9.91518795e-01 9.98180270e-01 9.89509463e-01\n",
            " 7.20985055e-01 9.99917150e-01 9.94949102e-01 3.49904507e-01\n",
            " 6.97915554e-02 9.97124612e-01 9.98745918e-01 9.90576029e-01\n",
            " 5.56502268e-02 9.54894841e-01 9.72558975e-01 9.98157322e-01\n",
            " 2.48842482e-02 9.59435999e-01 9.10986006e-01 1.78167939e-01\n",
            " 6.70698360e-02 2.02610925e-01 9.97774184e-01 7.22283944e-02\n",
            " 8.45676009e-03 1.77747220e-01 3.62726562e-02 4.57282215e-02\n",
            " 9.96659279e-01 9.99941468e-01 6.25670314e-01 9.20143604e-01\n",
            " 9.99993324e-01 9.99977469e-01]\n",
            "predict [0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1.\n",
            " 1. 1. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 1. 1. 1. 1. 0. 0. 0. 0. 0. 1. 1.\n",
            " 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1.\n",
            " 0. 1. 1. 1. 0. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1.]\n",
            "Train Epoch: 26 [0/54 (0%)]\tTrain Loss: 0.008955\n",
            "Train Epoch: 26 [10/54 (19%)]\tTrain Loss: 0.002417\n",
            "Train Epoch: 26 [20/54 (37%)]\tTrain Loss: 0.000109\n",
            "Train Epoch: 26 [30/54 (56%)]\tTrain Loss: 0.018834\n",
            "Train Epoch: 26 [40/54 (74%)]\tTrain Loss: 0.001948\n",
            "Train Epoch: 26 [50/54 (93%)]\tTrain Loss: 0.006632\n",
            "\n",
            "Train set: Average loss: 0.0078, Accuracy: 414/425 (97%)\n",
            "\n",
            "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            "score [2.23691063e-03 1.39729485e-01 2.11747624e-02 3.86740379e-02\n",
            " 1.71169522e-03 1.69161148e-03 2.21742183e-01 6.93113776e-03\n",
            " 4.11911458e-02 9.17421654e-02 2.94671237e-01 1.72302481e-02\n",
            " 7.36765414e-02 2.99609806e-02 5.44185415e-02 2.57982523e-04\n",
            " 4.19071643e-03 1.71696886e-01 3.89533490e-02 2.56777625e-03\n",
            " 1.75895005e-01 3.13944995e-01 6.21477723e-01 9.98415589e-01\n",
            " 6.75160408e-01 2.91486979e-01 9.70610619e-01 2.08644420e-02\n",
            " 6.66043069e-03 1.04324585e-02 2.56863803e-01 3.34628016e-01\n",
            " 5.48823513e-02 7.61685485e-04 1.10734918e-03 9.50735901e-03\n",
            " 5.21171885e-03 1.09903635e-02 9.98409450e-01 9.55232143e-01\n",
            " 9.93057489e-01 9.32770431e-01 2.67540049e-02 1.00333495e-02\n",
            " 3.37303951e-02 5.13069592e-02 9.85144824e-02 1.60651892e-01\n",
            " 6.39841318e-01 8.82873476e-01 9.46721137e-01 6.64065534e-04\n",
            " 1.09013272e-04 1.44932972e-04 1.97096402e-03 1.12131052e-03\n",
            " 7.30085224e-02 1.34973598e-04 6.74406838e-05 1.56473217e-03\n",
            " 1.00000000e+00 1.00000000e+00 9.99999285e-01 1.00000000e+00\n",
            " 7.10220397e-01 9.99907970e-01 9.99961019e-01 1.00000000e+00\n",
            " 1.00000000e+00 9.91923630e-01 2.61446208e-01 1.02849931e-01\n",
            " 9.99885798e-01 9.97513771e-01 9.66728210e-01 9.97963667e-01\n",
            " 9.99737799e-01 9.99981999e-01 9.99999881e-01 9.99865890e-01\n",
            " 9.99997735e-01 1.00000000e+00 1.00000000e+00 9.98518169e-01\n",
            " 2.44623616e-01 9.93090570e-01 9.95114446e-01 6.26745701e-01\n",
            " 6.46996856e-01 9.99999881e-01 9.99815524e-01 3.50455232e-02\n",
            " 1.48075670e-02 9.90665972e-01 9.99969482e-01 9.86742496e-01\n",
            " 7.31334928e-03 9.99941230e-01 9.98075843e-01 9.99980330e-01\n",
            " 9.60706115e-01 9.95327711e-01 9.98889744e-01 2.02274825e-02\n",
            " 5.54765901e-03 1.07320935e-01 9.84945774e-01 6.17259406e-02\n",
            " 3.39590237e-02 7.17057064e-02 5.12049645e-02 6.85181201e-01\n",
            " 9.73338068e-01 1.00000000e+00 8.24725926e-01 9.81166661e-01\n",
            " 1.00000000e+00 9.99999881e-01]\n",
            "predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1.\n",
            " 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0.\n",
            " 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1.\n",
            " 0. 1. 1. 1. 1. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1.]\n",
            "Train Epoch: 27 [0/54 (0%)]\tTrain Loss: 0.008510\n",
            "Train Epoch: 27 [10/54 (19%)]\tTrain Loss: 0.000255\n",
            "Train Epoch: 27 [20/54 (37%)]\tTrain Loss: 0.006198\n",
            "Train Epoch: 27 [30/54 (56%)]\tTrain Loss: 0.000572\n",
            "Train Epoch: 27 [40/54 (74%)]\tTrain Loss: 0.000494\n",
            "Train Epoch: 27 [50/54 (93%)]\tTrain Loss: 0.029863\n",
            "\n",
            "Train set: Average loss: 0.0035, Accuracy: 420/425 (99%)\n",
            "\n",
            "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            "score [6.79444522e-04 2.50511736e-01 1.81839112e-02 9.51885129e-04\n",
            " 2.60658679e-04 6.99728087e-04 2.82456100e-01 6.57345925e-04\n",
            " 1.27889961e-02 5.81638813e-02 1.61437437e-01 1.13806147e-02\n",
            " 3.69937122e-02 6.71051210e-03 2.13484354e-02 1.82238564e-05\n",
            " 1.51712785e-03 5.04243411e-02 9.67820175e-03 1.14275876e-03\n",
            " 2.58430332e-01 4.93201077e-01 8.53490472e-01 9.98628616e-01\n",
            " 5.49655974e-01 2.00206116e-01 9.57447469e-01 7.98703078e-03\n",
            " 2.70972890e-03 6.15756633e-03 6.50573373e-02 4.19064313e-02\n",
            " 2.64185630e-02 3.74887895e-05 1.12161244e-04 1.66938687e-03\n",
            " 1.14911282e-03 4.38748067e-03 1.69304490e-01 5.11686020e-02\n",
            " 3.46300185e-01 2.81003237e-01 1.56289544e-02 1.77250057e-02\n",
            " 7.34124362e-01 3.54978554e-02 2.08675355e-01 1.80299923e-01\n",
            " 6.40819609e-01 6.82769001e-01 9.63937461e-01 2.85626855e-04\n",
            " 1.04999408e-05 3.72670802e-05 4.26380982e-04 4.99532398e-05\n",
            " 2.01663151e-01 1.30767276e-05 9.92378682e-06 1.09177025e-03\n",
            " 9.99938965e-01 9.99856114e-01 9.99441803e-01 9.99278486e-01\n",
            " 8.83032680e-01 9.95235860e-01 9.95719969e-01 9.99976993e-01\n",
            " 9.99999642e-01 9.43194091e-01 7.25074634e-02 3.07056885e-02\n",
            " 9.97568548e-01 9.59638715e-01 6.74596906e-01 9.90013361e-01\n",
            " 9.99711215e-01 9.99960542e-01 9.99461710e-01 9.95623767e-01\n",
            " 9.99362886e-01 9.99994397e-01 9.99999881e-01 9.10710990e-01\n",
            " 3.29806730e-02 9.17886615e-01 9.92931485e-01 6.53827608e-01\n",
            " 7.35256016e-01 9.86778378e-01 9.43608522e-01 2.84237955e-02\n",
            " 1.71010045e-03 9.90704358e-01 9.99769747e-01 9.97257948e-01\n",
            " 6.15947740e-03 9.70914721e-01 8.86205077e-01 9.79903162e-01\n",
            " 7.16352314e-02 3.96154791e-01 5.59787214e-01 6.84771314e-03\n",
            " 1.98526890e-03 9.97511018e-03 9.92862880e-01 5.46192154e-02\n",
            " 1.73470424e-03 2.29391437e-02 2.36066338e-02 9.89628211e-02\n",
            " 9.24328148e-01 9.99994397e-01 8.05928558e-03 1.29051032e-02\n",
            " 9.99991775e-01 9.99939322e-01]\n",
            "predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1.\n",
            " 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.\n",
            " 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1.\n",
            " 0. 1. 1. 1. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 1.]\n",
            "Train Epoch: 28 [0/54 (0%)]\tTrain Loss: 0.000255\n",
            "Train Epoch: 28 [10/54 (19%)]\tTrain Loss: 0.005777\n",
            "Train Epoch: 28 [20/54 (37%)]\tTrain Loss: 0.000251\n",
            "Train Epoch: 28 [30/54 (56%)]\tTrain Loss: 0.000126\n",
            "Train Epoch: 28 [40/54 (74%)]\tTrain Loss: 0.000345\n",
            "Train Epoch: 28 [50/54 (93%)]\tTrain Loss: 0.001046\n",
            "\n",
            "Train set: Average loss: 0.0078, Accuracy: 420/425 (99%)\n",
            "\n",
            "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            "score [1.07065577e-03 8.19572985e-01 3.62967439e-02 3.39204911e-03\n",
            " 1.28818292e-03 1.04037498e-03 5.34946978e-01 3.00033228e-03\n",
            " 5.98716550e-03 1.35843484e-02 1.18011765e-01 5.32637117e-03\n",
            " 5.71970195e-02 3.18849133e-03 1.07880170e-02 4.50315383e-05\n",
            " 2.30315444e-03 2.28127767e-03 7.07915872e-02 7.76268030e-03\n",
            " 3.43484282e-01 7.34135732e-02 9.99307394e-01 9.99984145e-01\n",
            " 3.44529115e-02 9.99676228e-01 9.80060101e-01 6.65200222e-03\n",
            " 4.52550733e-03 5.27396146e-03 2.36641504e-02 2.61287749e-01\n",
            " 5.86244240e-02 1.75820838e-04 3.99465120e-04 5.98177034e-03\n",
            " 2.62207701e-03 4.36153375e-02 5.57009131e-03 3.24996226e-02\n",
            " 1.63736001e-01 1.98233992e-01 5.09266444e-02 2.20213190e-01\n",
            " 9.28966463e-01 2.02785417e-01 9.52445865e-01 8.23516846e-01\n",
            " 7.22631693e-01 9.15624261e-01 9.74367797e-01 3.94678151e-04\n",
            " 4.01492871e-05 1.18506709e-04 3.61912855e-04 2.45063839e-05\n",
            " 8.97016469e-03 4.47448183e-05 3.04942587e-05 4.11990099e-03\n",
            " 9.99999285e-01 9.99898911e-01 9.99613583e-01 9.99953389e-01\n",
            " 8.62092137e-01 9.66938019e-01 9.28894758e-01 1.00000000e+00\n",
            " 1.00000000e+00 9.90035892e-01 2.83459038e-01 1.40538365e-01\n",
            " 9.99999166e-01 9.41092014e-01 4.30865586e-01 9.99998927e-01\n",
            " 9.99965668e-01 9.99998927e-01 1.00000000e+00 9.99904752e-01\n",
            " 1.00000000e+00 1.00000000e+00 1.00000000e+00 6.44138828e-02\n",
            " 1.52729861e-02 9.97167647e-01 9.99967575e-01 9.90446925e-01\n",
            " 2.72158563e-01 1.00000000e+00 9.98232722e-01 2.12440193e-02\n",
            " 4.16705525e-03 9.95236635e-01 9.99981284e-01 9.99882340e-01\n",
            " 4.50176140e-03 2.77859837e-01 9.95754957e-01 9.99655008e-01\n",
            " 2.67439429e-03 1.42576724e-01 6.64607361e-02 7.12754391e-03\n",
            " 5.08915540e-03 7.95516651e-03 9.94991720e-01 1.74059011e-02\n",
            " 9.79462871e-04 1.95843149e-02 2.37174286e-03 7.83947110e-03\n",
            " 9.06343400e-01 1.00000000e+00 1.50179071e-02 2.59721696e-01\n",
            " 9.99918342e-01 9.99989152e-01]\n",
            "predict [0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1.\n",
            " 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1.\n",
            " 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0.\n",
            " 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 0. 1. 1. 0. 0. 1. 1. 1.\n",
            " 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 1.]\n",
            "Train Epoch: 29 [0/54 (0%)]\tTrain Loss: 0.000547\n",
            "Train Epoch: 29 [10/54 (19%)]\tTrain Loss: 0.034910\n",
            "Train Epoch: 29 [20/54 (37%)]\tTrain Loss: 0.000246\n",
            "Train Epoch: 29 [30/54 (56%)]\tTrain Loss: 0.036792\n",
            "Train Epoch: 29 [40/54 (74%)]\tTrain Loss: 0.000336\n",
            "Train Epoch: 29 [50/54 (93%)]\tTrain Loss: 0.052967\n",
            "\n",
            "Train set: Average loss: 0.0129, Accuracy: 413/425 (97%)\n",
            "\n",
            "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            "score [6.97421981e-03 9.52240527e-02 1.79370213e-02 4.06226050e-03\n",
            " 4.44765552e-04 4.80758678e-03 3.19187582e-01 4.94674640e-03\n",
            " 2.81870440e-02 3.68234552e-02 2.77737767e-01 1.47295957e-02\n",
            " 3.21611583e-01 9.36316792e-03 6.47898838e-02 1.46943785e-04\n",
            " 2.50279391e-03 5.44854300e-03 4.42459695e-02 1.68842881e-03\n",
            " 1.61095366e-01 8.23372185e-01 9.16728735e-01 9.99995947e-01\n",
            " 9.58696604e-01 9.96447682e-01 9.92658317e-01 8.41328781e-03\n",
            " 9.74066090e-03 1.40434029e-02 1.45590082e-01 3.53935093e-01\n",
            " 1.20278738e-01 1.51432192e-04 4.42545919e-04 2.89458223e-03\n",
            " 2.28947448e-03 6.43884465e-02 1.26869949e-02 1.09984145e-01\n",
            " 7.81562269e-01 9.33910966e-01 1.90147117e-01 2.88435578e-01\n",
            " 3.68152618e-01 3.66873771e-01 9.87968564e-01 9.14290786e-01\n",
            " 8.87531519e-01 9.96981919e-01 9.98235822e-01 1.38254996e-04\n",
            " 6.91386231e-06 5.25657779e-05 7.36835238e-04 1.35432319e-05\n",
            " 7.86345005e-02 8.02249779e-05 1.45268023e-05 8.10430292e-03\n",
            " 9.99998689e-01 9.99985933e-01 9.99997854e-01 9.99998212e-01\n",
            " 9.17887509e-01 9.98464942e-01 9.99286354e-01 1.00000000e+00\n",
            " 9.99984622e-01 9.95558858e-01 6.73417971e-02 8.49932209e-02\n",
            " 9.99862671e-01 9.78309214e-01 6.08765781e-01 9.99963164e-01\n",
            " 9.99999523e-01 9.99999285e-01 1.00000000e+00 9.99772966e-01\n",
            " 9.99996185e-01 1.00000000e+00 1.00000000e+00 9.91893828e-01\n",
            " 6.30225539e-01 9.99956608e-01 9.99999642e-01 9.99407291e-01\n",
            " 8.81642938e-01 9.99999881e-01 9.99962449e-01 3.27118456e-01\n",
            " 1.37018841e-02 9.99999762e-01 1.00000000e+00 9.99997973e-01\n",
            " 1.61164086e-02 3.06021780e-01 3.50121975e-01 9.99823987e-01\n",
            " 9.17050149e-03 9.75023985e-01 2.28561163e-01 5.45491986e-02\n",
            " 1.21045131e-02 2.72616655e-01 9.99999166e-01 1.20847963e-01\n",
            " 2.62268772e-03 3.59682292e-02 1.11897057e-02 1.83621496e-02\n",
            " 9.87595201e-01 1.00000000e+00 9.95990694e-01 9.99840975e-01\n",
            " 9.99994040e-01 1.00000000e+00]\n",
            "predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1.\n",
            " 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 1. 1.\n",
            " 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1.\n",
            " 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1.]\n",
            "Train Epoch: 30 [0/54 (0%)]\tTrain Loss: 0.006682\n",
            "Train Epoch: 30 [10/54 (19%)]\tTrain Loss: 0.000260\n",
            "Train Epoch: 30 [20/54 (37%)]\tTrain Loss: 0.011287\n",
            "Train Epoch: 30 [30/54 (56%)]\tTrain Loss: 0.000674\n",
            "Train Epoch: 30 [40/54 (74%)]\tTrain Loss: 0.000051\n",
            "Train Epoch: 30 [50/54 (93%)]\tTrain Loss: 0.001240\n",
            "\n",
            "Train set: Average loss: 0.0052, Accuracy: 422/425 (99%)\n",
            "\n",
            "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            "score [6.62876805e-03 3.07515208e-02 1.80657357e-02 9.74813942e-04\n",
            " 2.07986341e-05 1.86106004e-03 4.90521669e-01 1.90126395e-03\n",
            " 2.79449709e-02 5.17358631e-02 6.11613333e-01 3.76964081e-03\n",
            " 4.01525587e-01 3.33563844e-03 1.10119758e-02 1.24984206e-06\n",
            " 3.95060561e-05 5.65732989e-05 3.35449837e-02 3.13221419e-04\n",
            " 2.53540099e-01 4.41903889e-01 7.97033012e-01 9.84862745e-01\n",
            " 6.55245006e-01 9.73040104e-01 7.67498195e-01 2.87353955e-02\n",
            " 8.67906213e-03 5.92619600e-03 4.04835761e-01 6.94070458e-01\n",
            " 1.83451742e-01 3.80702136e-06 1.29553518e-06 7.54207198e-04\n",
            " 5.07180230e-04 1.68064922e-01 1.50690395e-02 2.99389660e-01\n",
            " 8.30921173e-01 7.18186021e-01 1.80909231e-01 1.11240685e-01\n",
            " 3.65163118e-01 9.73384902e-02 9.64632571e-01 3.89787853e-01\n",
            " 6.63330615e-01 9.53157008e-01 9.76217210e-01 4.71084559e-06\n",
            " 4.68425270e-07 7.40493874e-07 3.61850398e-04 1.34569757e-06\n",
            " 1.03961369e-02 8.23852474e-07 1.75030414e-06 1.24316907e-03\n",
            " 9.98751163e-01 9.98601377e-01 9.99172628e-01 9.99220133e-01\n",
            " 7.62614012e-01 9.98858690e-01 9.94748056e-01 9.99863148e-01\n",
            " 9.99136508e-01 9.76062477e-01 1.80887133e-01 1.54515743e-01\n",
            " 9.84108984e-01 8.82350564e-01 9.26306248e-01 9.97462749e-01\n",
            " 9.99311566e-01 9.99385357e-01 9.99969959e-01 9.82474864e-01\n",
            " 9.89882886e-01 9.99883771e-01 9.99827206e-01 9.47067738e-01\n",
            " 6.91699922e-01 9.95451748e-01 9.96808469e-01 9.58753824e-01\n",
            " 8.39259326e-02 9.98818457e-01 9.90507603e-01 4.75805700e-01\n",
            " 2.62319692e-03 9.96867001e-01 9.99555290e-01 9.88957763e-01\n",
            " 6.63154665e-03 4.37926054e-01 6.46846592e-01 9.86679852e-01\n",
            " 1.53424579e-03 6.99220598e-01 2.36942738e-01 6.53990218e-03\n",
            " 4.62673511e-03 1.47066906e-01 9.96437907e-01 3.14641111e-02\n",
            " 1.68769213e-04 1.86499804e-02 1.00557634e-03 2.20658071e-02\n",
            " 9.48959053e-01 9.99584258e-01 8.04118931e-01 9.81058896e-01\n",
            " 9.98499393e-01 9.99110401e-01]\n",
            "predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1.\n",
            " 1. 1. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 1. 0.\n",
            " 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 0. 0. 1. 1. 1.\n",
            " 0. 0. 1. 1. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1.]\n",
            "vote_pred [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1.\n",
            " 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 1.\n",
            " 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1.\n",
            " 0. 1. 1. 1. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 1. 1. 0. 1. 1.]\n",
            "targetlist [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            "TP= 41 TN= 49 FN= 17 FP= 11\n",
            "TP+FP 52\n",
            "precision 0.7884615384615384\n",
            "recall 0.7068965517241379\n",
            "F1 0.7454545454545455\n",
            "acc 0.7627118644067796\n",
            "AUCp 0.7617816091954022\n",
            "AUC 0.8540229885057471\n",
            "\n",
            " The epoch is 30, average recall: 0.7069, average precision: 0.7885,average F1: 0.7455, average accuracy: 0.7627, average AUC: 0.8540\n",
            "Train Epoch: 31 [0/54 (0%)]\tTrain Loss: 0.000106\n",
            "Train Epoch: 31 [10/54 (19%)]\tTrain Loss: 0.002043\n",
            "Train Epoch: 31 [20/54 (37%)]\tTrain Loss: 0.000517\n",
            "Train Epoch: 31 [30/54 (56%)]\tTrain Loss: 0.002912\n",
            "Train Epoch: 31 [40/54 (74%)]\tTrain Loss: 0.000876\n",
            "Train Epoch: 31 [50/54 (93%)]\tTrain Loss: 0.010479\n",
            "\n",
            "Train set: Average loss: 0.0087, Accuracy: 418/425 (98%)\n",
            "\n",
            "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            "score [6.14941493e-03 8.08871724e-03 1.04674809e-02 1.01896059e-02\n",
            " 6.91108871e-05 1.99823603e-02 8.18786696e-02 4.08608913e-02\n",
            " 2.47416750e-01 6.44185841e-02 6.62860930e-01 2.51436941e-02\n",
            " 4.95139092e-01 1.96437147e-02 1.03400394e-01 3.03315630e-07\n",
            " 8.50194465e-06 2.71179189e-04 1.67664200e-01 2.74459919e-04\n",
            " 1.02941483e-01 6.01066768e-01 3.24454755e-01 9.93495047e-01\n",
            " 9.15410638e-01 9.54883337e-01 9.78215992e-01 1.42141441e-02\n",
            " 1.75750756e-04 7.02153367e-04 2.43576273e-01 1.99662611e-01\n",
            " 2.25097775e-01 1.12905313e-04 3.16197344e-07 3.91888578e-04\n",
            " 8.23971568e-05 3.46789420e-01 5.64643648e-03 1.11756902e-02\n",
            " 3.91352594e-01 7.43461013e-01 2.05899596e-01 4.93548885e-02\n",
            " 8.90777539e-03 4.58567142e-02 9.84078646e-01 7.39554405e-01\n",
            " 7.74730206e-01 9.84446347e-01 9.76948559e-01 1.89702371e-06\n",
            " 7.45510746e-08 2.22104362e-08 5.80765016e-04 1.09201619e-06\n",
            " 2.02680275e-01 6.48284413e-08 1.61871139e-06 4.54012956e-03\n",
            " 9.99981880e-01 9.99777377e-01 9.99794304e-01 9.99360144e-01\n",
            " 6.26265109e-01 9.97284889e-01 9.97950017e-01 1.00000000e+00\n",
            " 9.99820411e-01 9.84094799e-01 5.37777424e-01 5.85367203e-01\n",
            " 8.99915457e-01 8.74176741e-01 6.48073137e-01 9.92547691e-01\n",
            " 9.99855161e-01 9.99629498e-01 1.00000000e+00 9.86583292e-01\n",
            " 9.98719573e-01 7.75079012e-01 1.00000000e+00 9.84781384e-01\n",
            " 7.81439483e-01 9.95922446e-01 9.99990821e-01 9.86262679e-01\n",
            " 1.16840355e-01 9.90349770e-01 9.87200379e-01 9.53367829e-01\n",
            " 2.66621690e-02 9.99990940e-01 9.99999881e-01 9.93628263e-01\n",
            " 4.46699597e-02 9.53001916e-01 9.14097190e-01 9.85408008e-01\n",
            " 1.83705643e-01 9.64600325e-01 5.48677027e-01 1.30250230e-01\n",
            " 1.89032890e-02 5.81147373e-01 9.99416709e-01 1.71328947e-01\n",
            " 7.83354789e-03 1.49978161e-01 7.58696795e-02 8.44163835e-01\n",
            " 9.92753804e-01 9.99476254e-01 9.17739511e-01 7.58211434e-01\n",
            " 9.99982357e-01 9.99588192e-01]\n",
            "predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1.\n",
            " 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1.\n",
            " 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 0. 1. 1. 1.\n",
            " 0. 1. 1. 1. 0. 1. 1. 0. 0. 1. 1. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1.]\n",
            "Train Epoch: 32 [0/54 (0%)]\tTrain Loss: 0.000198\n",
            "Train Epoch: 32 [10/54 (19%)]\tTrain Loss: 0.000232\n",
            "Train Epoch: 32 [20/54 (37%)]\tTrain Loss: 0.000154\n",
            "Train Epoch: 32 [30/54 (56%)]\tTrain Loss: 0.005347\n",
            "Train Epoch: 32 [40/54 (74%)]\tTrain Loss: 0.000090\n",
            "Train Epoch: 32 [50/54 (93%)]\tTrain Loss: 0.002905\n",
            "\n",
            "Train set: Average loss: 0.0037, Accuracy: 422/425 (99%)\n",
            "\n",
            "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            "score [6.27977122e-03 1.96799543e-02 4.29684930e-02 9.62326676e-03\n",
            " 3.54742879e-05 1.40205817e-02 2.00546280e-01 1.45374641e-01\n",
            " 3.10657620e-01 4.43124771e-02 6.50445879e-01 1.65319443e-02\n",
            " 4.99098510e-01 2.14605294e-02 5.29651679e-02 6.17474460e-10\n",
            " 2.46339619e-06 8.71185111e-05 8.17259997e-02 2.73580255e-04\n",
            " 1.37628913e-01 2.25104883e-01 3.77155900e-01 9.96765733e-01\n",
            " 7.63191581e-01 9.80694413e-01 9.88719285e-01 3.71455327e-02\n",
            " 3.56205070e-04 8.20989429e-04 7.10247159e-01 4.80933100e-01\n",
            " 7.63976350e-02 2.00879214e-07 3.61547814e-09 1.45536644e-04\n",
            " 1.66576137e-05 1.56352252e-01 8.18006396e-01 7.69912958e-01\n",
            " 9.83146131e-01 9.57037747e-01 2.28402495e-01 7.60556385e-02\n",
            " 2.05623940e-01 2.17848998e-02 9.77087200e-01 8.01676393e-01\n",
            " 5.65699399e-01 9.90065038e-01 9.71819818e-01 2.01059482e-12\n",
            " 1.00604258e-13 3.76477603e-13 1.51529832e-06 2.87369267e-10\n",
            " 7.19111552e-03 1.85485689e-11 4.20113766e-09 3.46114975e-04\n",
            " 9.98005450e-01 9.98506069e-01 9.98103857e-01 9.98679221e-01\n",
            " 9.79958594e-01 9.99427557e-01 9.97294605e-01 9.99842525e-01\n",
            " 9.99608219e-01 9.95874107e-01 2.62504607e-01 7.68254042e-01\n",
            " 9.89082813e-01 9.42064524e-01 9.88207877e-01 9.96863961e-01\n",
            " 9.98736918e-01 9.99133289e-01 9.99999881e-01 9.75341976e-01\n",
            " 9.96850193e-01 9.95548010e-01 9.99518394e-01 9.85643387e-01\n",
            " 4.53240424e-01 9.92730916e-01 9.97565508e-01 9.73077476e-01\n",
            " 2.69687742e-01 9.95939136e-01 9.98254836e-01 2.69932002e-01\n",
            " 4.00025351e-03 9.99867797e-01 9.99978900e-01 9.94430244e-01\n",
            " 3.64658190e-04 9.61004674e-01 9.33514714e-01 9.96378124e-01\n",
            " 2.70526439e-01 9.57786560e-01 9.93276775e-01 2.35070402e-04\n",
            " 2.23986266e-04 3.23652029e-01 9.97505486e-01 1.84533149e-02\n",
            " 9.85480938e-03 1.76666811e-01 1.10482303e-02 9.12078917e-01\n",
            " 9.69019711e-01 9.99659777e-01 9.71121967e-01 9.74813461e-01\n",
            " 9.99912500e-01 9.99929070e-01]\n",
            "predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.\n",
            " 1. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 0. 0. 0. 0. 1. 1.\n",
            " 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 0. 1. 1. 0. 0. 1. 1. 1.\n",
            " 0. 1. 1. 1. 0. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1.]\n",
            "Train Epoch: 33 [0/54 (0%)]\tTrain Loss: 0.000324\n",
            "Train Epoch: 33 [10/54 (19%)]\tTrain Loss: 0.001146\n",
            "Train Epoch: 33 [20/54 (37%)]\tTrain Loss: 0.002048\n",
            "Train Epoch: 33 [30/54 (56%)]\tTrain Loss: 0.001527\n",
            "Train Epoch: 33 [40/54 (74%)]\tTrain Loss: 0.044486\n",
            "Train Epoch: 33 [50/54 (93%)]\tTrain Loss: 0.007652\n",
            "\n",
            "Train set: Average loss: 0.0073, Accuracy: 421/425 (99%)\n",
            "\n",
            "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            "score [2.49954988e-03 3.05543235e-03 7.70767312e-03 9.10776900e-04\n",
            " 6.92068425e-05 1.39065953e-02 6.52868450e-02 3.14554269e-03\n",
            " 1.14897098e-02 7.10710287e-02 5.53874195e-01 4.88098264e-02\n",
            " 3.97620082e-01 1.40815944e-04 8.10500118e-04 2.41167982e-05\n",
            " 1.59271731e-04 1.63040489e-01 7.03506684e-03 8.56636092e-04\n",
            " 8.19473788e-02 7.18298972e-01 6.76239049e-03 9.95597064e-01\n",
            " 9.05567229e-01 6.42093062e-01 9.94184792e-01 4.79235809e-04\n",
            " 1.18920743e-03 1.25094911e-03 5.19148350e-01 7.91374687e-03\n",
            " 8.72105826e-03 1.59545933e-04 1.18910815e-04 8.86111695e-04\n",
            " 4.53188317e-04 1.59783050e-01 5.42948954e-02 4.23335796e-03\n",
            " 1.90357920e-02 5.79038620e-01 3.15001085e-02 3.05188447e-03\n",
            " 4.85864608e-03 6.55158702e-03 5.61873317e-01 7.13245213e-01\n",
            " 2.55242050e-01 9.78421688e-01 9.19076920e-01 2.39645196e-05\n",
            " 1.97406072e-04 8.01968345e-05 4.47307192e-02 2.87707965e-03\n",
            " 4.04148668e-01 1.26872095e-04 4.18288354e-03 4.32053767e-03\n",
            " 9.99999642e-01 9.99956131e-01 9.99890924e-01 9.99999285e-01\n",
            " 3.24814588e-01 9.18631673e-01 2.53728807e-01 9.98637497e-01\n",
            " 9.80385423e-01 9.63114500e-01 3.88110355e-02 5.61238714e-02\n",
            " 2.00885430e-01 1.03824437e-02 5.88676393e-01 8.74331713e-01\n",
            " 9.93596196e-01 9.98976350e-01 1.00000000e+00 1.53172957e-02\n",
            " 1.49022471e-02 6.38556659e-01 9.99957681e-01 9.97734547e-01\n",
            " 9.81215894e-01 9.43022251e-01 9.99799192e-01 6.94115937e-01\n",
            " 1.03281960e-02 2.43231297e-01 9.60379899e-01 2.37286906e-03\n",
            " 3.87669774e-04 9.98822391e-01 9.99973416e-01 7.75773227e-01\n",
            " 1.56014704e-03 9.80481923e-01 3.38435441e-01 9.99440134e-01\n",
            " 1.54288523e-02 8.82737994e-01 9.32330251e-01 4.46248706e-03\n",
            " 5.83731162e-04 1.57718569e-01 9.94872153e-01 7.98315763e-01\n",
            " 1.03873201e-03 1.40126487e-02 1.03218868e-01 3.07880640e-01\n",
            " 2.28788957e-01 8.42148438e-02 9.39477623e-01 9.73896265e-01\n",
            " 1.00000000e+00 1.00000000e+00]\n",
            "predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1.\n",
            " 1. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1.\n",
            " 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 0. 1. 0. 1. 1. 1. 0. 0.\n",
            " 0. 0. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 0. 0. 1. 1. 1.\n",
            " 0. 1. 0. 1. 0. 1. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1.]\n",
            "Train Epoch: 34 [0/54 (0%)]\tTrain Loss: 0.000030\n",
            "Train Epoch: 34 [10/54 (19%)]\tTrain Loss: 0.000659\n",
            "Train Epoch: 34 [20/54 (37%)]\tTrain Loss: 0.000533\n",
            "Train Epoch: 34 [30/54 (56%)]\tTrain Loss: 0.000718\n",
            "Train Epoch: 34 [40/54 (74%)]\tTrain Loss: 0.000407\n",
            "Train Epoch: 34 [50/54 (93%)]\tTrain Loss: 0.000202\n",
            "\n",
            "Train set: Average loss: 0.0051, Accuracy: 420/425 (99%)\n",
            "\n",
            "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            "score [8.74952704e-04 9.29161236e-02 4.32877280e-02 4.03924007e-03\n",
            " 6.98645657e-04 3.95968556e-03 9.00572062e-01 1.36167556e-02\n",
            " 2.84054335e-02 4.67618890e-02 2.53965765e-01 2.97776125e-02\n",
            " 1.31240234e-01 3.37340645e-02 2.78840303e-01 1.06139469e-05\n",
            " 1.55434347e-04 5.41748881e-01 2.52620459e-01 2.34674644e-02\n",
            " 5.74341595e-01 5.39730906e-01 8.73142123e-01 9.99888659e-01\n",
            " 6.15897000e-01 9.99901533e-01 9.99704897e-01 3.92247826e-01\n",
            " 1.51486173e-02 2.24728347e-03 9.98222768e-01 8.71324539e-01\n",
            " 8.82716775e-02 1.11271933e-04 3.29896575e-05 1.87411939e-03\n",
            " 6.57290220e-04 1.16798244e-02 9.48989093e-01 2.20264541e-03\n",
            " 3.68383154e-02 4.48895603e-01 1.28715970e-02 8.47934105e-04\n",
            " 9.87119198e-01 2.57885107e-03 5.23988545e-01 1.73560306e-01\n",
            " 2.49012008e-01 9.89217758e-01 8.69822502e-01 2.48254878e-06\n",
            " 1.06826730e-04 9.31481918e-06 7.19955489e-02 3.53316282e-04\n",
            " 5.15512884e-01 3.29395807e-05 5.78992185e-04 1.43697262e-01\n",
            " 9.99989033e-01 9.99962449e-01 9.99849081e-01 9.99256313e-01\n",
            " 9.99529243e-01 9.99759257e-01 9.99600470e-01 9.98846412e-01\n",
            " 9.99782383e-01 9.91526544e-01 2.17929065e-01 8.28231871e-01\n",
            " 9.99941468e-01 9.31660116e-01 9.99596536e-01 9.99727070e-01\n",
            " 9.99824703e-01 9.99941230e-01 9.99999881e-01 8.03377748e-01\n",
            " 9.99030232e-01 9.99999881e-01 9.99988198e-01 9.03739333e-01\n",
            " 3.43692213e-01 8.92693400e-01 9.99992728e-01 9.89131570e-01\n",
            " 9.86125648e-01 9.92871702e-01 9.99994755e-01 7.01124081e-03\n",
            " 6.88332133e-04 9.99999881e-01 1.00000000e+00 9.99959111e-01\n",
            " 1.41647067e-02 9.23332334e-01 9.07377303e-01 9.24881041e-01\n",
            " 5.96977510e-02 9.91406322e-01 9.99799669e-01 6.66008331e-03\n",
            " 2.39465572e-03 5.96107543e-01 9.99865294e-01 4.19591784e-01\n",
            " 5.58428513e-03 5.73895164e-02 4.12734509e-01 2.38020539e-01\n",
            " 9.75791693e-01 9.99547184e-01 9.99917984e-01 9.99651313e-01\n",
            " 9.99998689e-01 9.99984145e-01]\n",
            "predict [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1. 1. 1.\n",
            " 1. 1. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 1. 0.\n",
            " 0. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1.\n",
            " 0. 1. 1. 1. 0. 1. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1.]\n",
            "Train Epoch: 35 [0/54 (0%)]\tTrain Loss: 0.000757\n",
            "Train Epoch: 35 [10/54 (19%)]\tTrain Loss: 0.012288\n",
            "Train Epoch: 35 [20/54 (37%)]\tTrain Loss: 0.001974\n",
            "Train Epoch: 35 [30/54 (56%)]\tTrain Loss: 0.008329\n",
            "Train Epoch: 35 [40/54 (74%)]\tTrain Loss: 0.000151\n",
            "Train Epoch: 35 [50/54 (93%)]\tTrain Loss: 0.000090\n",
            "\n",
            "Train set: Average loss: 0.0039, Accuracy: 421/425 (99%)\n",
            "\n",
            "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            "score [5.56170009e-04 6.65517058e-04 1.27170619e-03 2.09786440e-03\n",
            " 9.59500467e-05 4.75601479e-03 1.61848664e-02 9.73351207e-03\n",
            " 1.41384415e-02 1.59644410e-02 5.98755717e-01 1.16788223e-02\n",
            " 1.18375435e-01 1.79853267e-03 5.76451886e-03 2.34566687e-06\n",
            " 5.02860421e-05 5.17424941e-02 2.06651297e-04 6.33741365e-05\n",
            " 2.69705057e-03 1.17350116e-01 1.21694028e-01 9.99978185e-01\n",
            " 2.84485132e-01 7.77575970e-01 9.94020641e-01 1.35511480e-04\n",
            " 3.46699526e-04 1.04812370e-03 7.08273530e-01 5.07353693e-02\n",
            " 1.96098313e-02 1.07207843e-05 1.14581189e-05 9.37573204e-04\n",
            " 2.73095269e-04 4.10484150e-02 1.01636961e-01 2.97680177e-04\n",
            " 2.31929426e-03 3.48690664e-03 7.67280012e-02 9.72679758e-04\n",
            " 9.86112654e-01 3.51307518e-03 9.80859399e-01 8.24830055e-01\n",
            " 6.28129721e-01 9.96042848e-01 9.59980488e-01 5.93165680e-07\n",
            " 1.63581126e-06 2.14366423e-06 9.86185740e-04 9.84477228e-05\n",
            " 4.24161088e-03 7.29717249e-06 3.52078750e-05 1.29525666e-03\n",
            " 1.00000000e+00 9.99980330e-01 9.99948144e-01 9.99989152e-01\n",
            " 9.95083213e-01 9.99998450e-01 9.98036563e-01 1.00000000e+00\n",
            " 9.99998331e-01 9.99541044e-01 5.29137850e-02 2.01776728e-01\n",
            " 9.99857545e-01 8.46981168e-01 9.88997757e-01 9.99894738e-01\n",
            " 9.99984503e-01 9.99989867e-01 9.99991179e-01 9.09402430e-01\n",
            " 9.97615457e-01 1.00000000e+00 1.00000000e+00 9.36835229e-01\n",
            " 1.24206496e-02 9.85609114e-01 9.99985456e-01 8.93273652e-01\n",
            " 7.50320591e-03 9.88191545e-01 9.57014799e-01 2.09059399e-02\n",
            " 9.95398848e-04 9.99955058e-01 1.00000000e+00 9.97188032e-01\n",
            " 2.22490844e-03 8.59734476e-01 1.76916391e-01 1.01347722e-01\n",
            " 4.42171469e-03 9.94842112e-01 9.98594940e-01 1.37492537e-03\n",
            " 2.84449052e-04 5.89696644e-03 9.98992264e-01 3.80612165e-01\n",
            " 1.09544944e-03 3.74516798e-03 2.15291589e-01 5.08778468e-02\n",
            " 9.25881028e-01 9.59472775e-01 9.81064975e-01 4.76060249e-02\n",
            " 1.00000000e+00 9.97682333e-01]\n",
            "predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.\n",
            " 0. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1.\n",
            " 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 0. 1. 1. 0. 0. 1. 1. 1.\n",
            " 0. 1. 0. 0. 0. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 1. 1. 0. 1. 1.]\n",
            "Train Epoch: 36 [0/54 (0%)]\tTrain Loss: 0.000100\n",
            "Train Epoch: 36 [10/54 (19%)]\tTrain Loss: 0.000064\n",
            "Train Epoch: 36 [20/54 (37%)]\tTrain Loss: 0.000130\n",
            "Train Epoch: 36 [30/54 (56%)]\tTrain Loss: 0.000054\n",
            "Train Epoch: 36 [40/54 (74%)]\tTrain Loss: 0.001706\n",
            "Train Epoch: 36 [50/54 (93%)]\tTrain Loss: 0.000352\n",
            "\n",
            "Train set: Average loss: 0.0031, Accuracy: 423/425 (100%)\n",
            "\n",
            "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            "score [5.38895023e-04 5.77917905e-04 1.11757603e-03 3.99942808e-02\n",
            " 1.19784533e-03 3.56372446e-03 1.66094322e-02 4.80537899e-02\n",
            " 5.47605716e-02 3.57137062e-02 7.93890238e-01 5.70799410e-02\n",
            " 3.01362514e-01 3.11018620e-03 1.07849773e-03 4.81090638e-06\n",
            " 4.37624076e-05 3.89889002e-01 3.07085691e-03 2.65047693e-05\n",
            " 3.06628412e-03 1.92923382e-01 4.43760380e-02 9.99904633e-01\n",
            " 4.11535650e-01 5.35232484e-01 9.96201813e-01 5.18838897e-05\n",
            " 1.34496047e-04 1.59306172e-03 1.34427790e-02 2.42501535e-02\n",
            " 1.28756752e-02 8.03918010e-06 9.41800408e-06 1.04558945e-03\n",
            " 3.04574642e-04 6.47680521e-01 7.47920796e-02 4.51647851e-04\n",
            " 5.33930119e-03 1.24515290e-03 1.01767056e-01 1.09031354e-03\n",
            " 9.91210341e-01 2.68500775e-01 9.98669624e-01 9.88378465e-01\n",
            " 9.54273820e-01 9.98683512e-01 9.87708688e-01 7.28683403e-07\n",
            " 8.61184390e-07 3.52301936e-06 6.36903290e-03 1.42525590e-04\n",
            " 7.00362958e-03 2.93676999e-06 5.56397754e-06 2.68052071e-02\n",
            " 1.00000000e+00 9.99966621e-01 9.99755085e-01 9.99865174e-01\n",
            " 9.83714879e-01 9.99008000e-01 9.95673001e-01 9.99996305e-01\n",
            " 9.99984622e-01 9.98074174e-01 2.56175488e-01 3.37065816e-01\n",
            " 9.99728620e-01 8.97094309e-01 7.76164412e-01 9.98064578e-01\n",
            " 9.99818861e-01 9.99987006e-01 9.99951482e-01 8.72595906e-01\n",
            " 9.99455750e-01 1.00000000e+00 9.99999046e-01 9.86162066e-01\n",
            " 5.66950142e-01 9.95454788e-01 9.99985218e-01 9.93636608e-01\n",
            " 5.20277675e-03 7.61297524e-01 7.73572922e-01 5.82731426e-01\n",
            " 9.34649818e-03 9.99987960e-01 1.00000000e+00 9.99976039e-01\n",
            " 8.53692740e-02 9.82695222e-01 3.32246095e-01 2.49641445e-02\n",
            " 1.85587145e-02 9.90655839e-01 9.69637990e-01 3.99158411e-02\n",
            " 1.09268557e-02 3.07811320e-01 9.99848604e-01 9.67178702e-01\n",
            " 8.31838846e-01 8.30599487e-01 9.81553674e-01 9.97025788e-01\n",
            " 9.99915004e-01 3.01618010e-01 1.55137628e-01 9.35773738e-03\n",
            " 1.00000000e+00 9.84283924e-01]\n",
            "predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.\n",
            " 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1.\n",
            " 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 0. 1. 1. 1.\n",
            " 0. 1. 0. 0. 0. 1. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 1. 1.]\n",
            "Train Epoch: 37 [0/54 (0%)]\tTrain Loss: 0.000566\n",
            "Train Epoch: 37 [10/54 (19%)]\tTrain Loss: 0.000391\n",
            "Train Epoch: 37 [20/54 (37%)]\tTrain Loss: 0.000330\n",
            "Train Epoch: 37 [30/54 (56%)]\tTrain Loss: 0.000255\n",
            "Train Epoch: 37 [40/54 (74%)]\tTrain Loss: 0.000074\n",
            "Train Epoch: 37 [50/54 (93%)]\tTrain Loss: 0.000062\n",
            "\n",
            "Train set: Average loss: 0.0026, Accuracy: 423/425 (100%)\n",
            "\n",
            "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            "score [2.90045273e-06 9.64364469e-01 7.67306760e-02 7.43839517e-02\n",
            " 7.83297699e-04 4.89428174e-04 9.95996356e-01 1.27774745e-01\n",
            " 6.33210182e-01 4.54518273e-02 9.52978671e-01 1.10907026e-01\n",
            " 8.94266725e-01 9.82745469e-01 9.92934883e-01 4.05091123e-05\n",
            " 2.97846436e-05 9.64777470e-01 9.37897861e-01 2.12045893e-01\n",
            " 7.33158350e-01 9.96949375e-01 9.79380131e-01 9.99962330e-01\n",
            " 9.96475041e-01 9.99270856e-01 9.99803483e-01 9.95733917e-01\n",
            " 1.40492935e-02 1.02982689e-02 9.98574138e-01 9.81150270e-01\n",
            " 6.31015062e-01 6.12496933e-08 5.42356382e-09 2.69373821e-04\n",
            " 1.47063383e-05 9.44853008e-01 9.86940682e-01 1.56280175e-01\n",
            " 6.18666530e-01 7.86800146e-01 1.45524606e-01 3.19037493e-03\n",
            " 9.96331871e-01 8.25326264e-01 9.98631060e-01 9.69165921e-01\n",
            " 9.84227240e-01 9.97997820e-01 9.96500850e-01 2.93733553e-08\n",
            " 2.71057252e-06 8.46968362e-07 7.28172064e-01 2.26948352e-04\n",
            " 7.17383087e-01 1.41588225e-06 4.58758294e-08 9.88171101e-01\n",
            " 9.99977827e-01 9.99841332e-01 9.99381781e-01 9.99858975e-01\n",
            " 9.99774396e-01 9.99418139e-01 9.93694007e-01 9.99965429e-01\n",
            " 9.99707639e-01 9.99604642e-01 9.55975413e-01 9.88760114e-01\n",
            " 9.99571264e-01 9.99301791e-01 9.99999523e-01 9.99887705e-01\n",
            " 9.99998689e-01 9.99997258e-01 9.99998689e-01 9.92949247e-01\n",
            " 9.99513149e-01 9.99999881e-01 9.99924064e-01 9.94782984e-01\n",
            " 9.89361107e-01 9.99862313e-01 9.99822438e-01 9.99832749e-01\n",
            " 9.97269213e-01 9.99969721e-01 9.99979973e-01 8.26021433e-01\n",
            " 1.16093881e-01 9.99966025e-01 1.00000000e+00 1.00000000e+00\n",
            " 9.75385487e-01 9.90649164e-01 9.93651330e-01 9.89604115e-01\n",
            " 1.06793575e-01 9.94979084e-01 9.98320043e-01 8.73845935e-01\n",
            " 1.75370112e-01 9.70927179e-01 9.99857306e-01 9.75871027e-01\n",
            " 8.61249387e-01 9.91503775e-01 9.97142255e-01 9.97016549e-01\n",
            " 9.99703228e-01 9.97855246e-01 9.99980688e-01 9.99999881e-01\n",
            " 9.99999762e-01 1.00000000e+00]\n",
            "predict [0. 1. 0. 0. 0. 0. 1. 0. 1. 0. 1. 0. 1. 1. 1. 0. 0. 1. 1. 0. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 0. 0. 1. 1. 1. 0. 0. 0. 0. 1. 1. 0. 1. 1. 0. 0. 1. 1. 1. 1.\n",
            " 1. 1. 1. 0. 0. 0. 1. 0. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1.\n",
            " 1. 1. 1. 1. 0. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            "Train Epoch: 38 [0/54 (0%)]\tTrain Loss: 0.000512\n",
            "Train Epoch: 38 [10/54 (19%)]\tTrain Loss: 0.000066\n",
            "Train Epoch: 38 [20/54 (37%)]\tTrain Loss: 0.000099\n",
            "Train Epoch: 38 [30/54 (56%)]\tTrain Loss: 0.000126\n",
            "Train Epoch: 38 [40/54 (74%)]\tTrain Loss: 0.000156\n",
            "Train Epoch: 38 [50/54 (93%)]\tTrain Loss: 0.007605\n",
            "\n",
            "Train set: Average loss: 0.0092, Accuracy: 419/425 (99%)\n",
            "\n",
            "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            "score [5.36492917e-06 7.17684161e-03 8.02944135e-03 1.53711624e-03\n",
            " 5.23303570e-05 1.00251011e-04 2.31706500e-02 1.57802389e-03\n",
            " 9.53696016e-03 2.73630098e-02 8.11353743e-01 1.74547955e-02\n",
            " 1.63429797e-01 1.14287641e-02 9.66907963e-02 7.87454326e-07\n",
            " 1.31363317e-03 6.84442446e-02 1.17883636e-02 1.97299663e-03\n",
            " 2.30250116e-02 2.96687037e-01 3.09324563e-02 9.99998808e-01\n",
            " 4.78253603e-01 4.88471419e-01 9.99899387e-01 1.25173721e-02\n",
            " 4.30885144e-03 1.08662844e-02 5.74117824e-02 5.27366670e-03\n",
            " 1.38488775e-02 5.97412181e-05 4.05570336e-06 1.75970490e-03\n",
            " 2.47185002e-04 1.69322714e-02 1.49823651e-01 2.64971075e-03\n",
            " 1.20122507e-02 2.76879333e-02 6.01061387e-03 6.33981312e-03\n",
            " 5.12439668e-01 4.85460134e-03 2.84261145e-02 3.00632603e-03\n",
            " 1.76637806e-02 8.07772756e-01 3.84787828e-01 1.41291366e-05\n",
            " 2.40148984e-05 5.17114677e-06 1.63833261e-03 3.47012654e-04\n",
            " 6.59039542e-02 3.24150969e-05 1.63748318e-05 1.78603479e-03\n",
            " 1.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
            " 9.73420203e-01 9.99999881e-01 7.72381544e-01 1.00000000e+00\n",
            " 1.00000000e+00 1.00000000e+00 6.72475174e-02 5.62841184e-02\n",
            " 9.32241678e-01 9.52788174e-01 9.48759079e-01 9.99933958e-01\n",
            " 1.00000000e+00 9.99997258e-01 1.00000000e+00 3.29831421e-01\n",
            " 4.62881595e-01 9.99949813e-01 1.00000000e+00 9.96670187e-01\n",
            " 3.08409091e-02 9.99205649e-01 9.99901056e-01 9.99996781e-01\n",
            " 5.09910025e-02 9.99999762e-01 9.94878054e-01 1.27003133e-01\n",
            " 1.79254916e-03 9.99944091e-01 1.00000000e+00 9.99728978e-01\n",
            " 4.03186791e-02 9.17739809e-01 9.82783437e-01 2.57303357e-01\n",
            " 3.82535234e-02 9.51505125e-01 9.37819958e-01 1.59898831e-03\n",
            " 2.97192135e-04 2.21549943e-02 9.99753654e-01 9.55530167e-01\n",
            " 1.29197955e-01 6.08453572e-01 9.69456613e-01 7.73575008e-01\n",
            " 9.99354780e-01 9.99983430e-01 4.94505197e-01 9.08701897e-01\n",
            " 1.00000000e+00 9.99993324e-01]\n",
            "predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.\n",
            " 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.\n",
            " 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0.\n",
            " 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 0. 1. 1. 1. 0. 1. 1. 0. 0. 1. 1. 1.\n",
            " 0. 1. 1. 0. 0. 1. 1. 0. 0. 0. 1. 1. 0. 1. 1. 1. 1. 1. 0. 1. 1. 1.]\n",
            "Train Epoch: 39 [0/54 (0%)]\tTrain Loss: 0.000460\n",
            "Train Epoch: 39 [10/54 (19%)]\tTrain Loss: 0.000123\n",
            "Train Epoch: 39 [20/54 (37%)]\tTrain Loss: 0.000031\n",
            "Train Epoch: 39 [30/54 (56%)]\tTrain Loss: 0.000120\n",
            "Train Epoch: 39 [40/54 (74%)]\tTrain Loss: 0.000076\n",
            "Train Epoch: 39 [50/54 (93%)]\tTrain Loss: 0.021321\n",
            "\n",
            "Train set: Average loss: 0.0045, Accuracy: 420/425 (99%)\n",
            "\n",
            "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            "score [8.05946092e-06 1.92540535e-03 4.23621153e-03 1.38249190e-03\n",
            " 5.75576087e-05 3.80441779e-05 3.58901136e-02 2.41251127e-03\n",
            " 4.66657281e-02 6.07849397e-02 7.10394919e-01 3.69560458e-02\n",
            " 1.10714696e-01 1.23185441e-02 2.11354449e-01 2.68821143e-08\n",
            " 3.91738104e-05 3.02030854e-02 3.54144693e-04 4.68910381e-04\n",
            " 1.31897442e-02 8.24184299e-01 7.55523816e-02 9.99985456e-01\n",
            " 8.60712826e-01 9.66519773e-01 9.99979973e-01 5.55164879e-03\n",
            " 1.14765356e-03 7.22072506e-03 4.52428944e-02 3.47337760e-02\n",
            " 1.09501109e-01 4.70720529e-09 1.14030611e-10 1.01085356e-03\n",
            " 1.97030153e-04 1.17173336e-01 9.71150771e-02 3.18554044e-03\n",
            " 6.00280128e-02 2.39596188e-01 6.61012344e-03 1.01030683e-02\n",
            " 7.24975348e-01 6.24912186e-03 1.02355383e-01 1.88340377e-02\n",
            " 1.54740423e-01 8.15086782e-01 8.36117506e-01 3.91977864e-08\n",
            " 4.39238939e-08 3.19963123e-09 2.41140588e-04 8.53481106e-06\n",
            " 2.14799400e-02 1.13974199e-08 1.71769021e-09 1.43168640e-04\n",
            " 9.99996185e-01 9.99432743e-01 9.99179184e-01 9.99892950e-01\n",
            " 9.57591414e-01 9.99971151e-01 6.96125805e-01 9.99999881e-01\n",
            " 1.00000000e+00 9.99220252e-01 1.50815487e-01 4.14563976e-02\n",
            " 9.21026826e-01 8.41586530e-01 9.82677877e-01 9.85082626e-01\n",
            " 9.99543726e-01 9.99874234e-01 9.96866286e-01 9.87164497e-01\n",
            " 9.98401105e-01 1.00000000e+00 1.00000000e+00 9.53182578e-01\n",
            " 7.94772059e-02 9.99905586e-01 9.99842048e-01 9.99721467e-01\n",
            " 2.38978527e-02 1.00000000e+00 9.99674201e-01 9.53764737e-01\n",
            " 4.21985378e-03 9.99994993e-01 1.00000000e+00 9.99996901e-01\n",
            " 1.99193247e-02 9.52743053e-01 9.29119766e-01 6.65858507e-01\n",
            " 4.24886383e-02 9.23750222e-01 9.76310253e-01 1.99061423e-03\n",
            " 4.72910091e-04 1.12288734e-02 9.99552071e-01 9.36360121e-01\n",
            " 6.04563951e-01 3.54886234e-01 9.88104284e-01 8.27988148e-01\n",
            " 9.99519944e-01 9.96258259e-01 1.57699138e-01 7.89050221e-01\n",
            " 1.00000000e+00 9.99975204e-01]\n",
            "predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1.\n",
            " 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.\n",
            " 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 0. 1. 1. 1. 0. 1. 1. 1.\n",
            " 0. 1. 1. 1. 0. 1. 1. 0. 0. 0. 1. 1. 1. 0. 1. 1. 1. 1. 0. 1. 1. 1.]\n",
            "Train Epoch: 40 [0/54 (0%)]\tTrain Loss: 0.000871\n",
            "Train Epoch: 40 [10/54 (19%)]\tTrain Loss: 0.000078\n",
            "Train Epoch: 40 [20/54 (37%)]\tTrain Loss: 0.000120\n",
            "Train Epoch: 40 [30/54 (56%)]\tTrain Loss: 0.000167\n",
            "Train Epoch: 40 [40/54 (74%)]\tTrain Loss: 0.000082\n",
            "Train Epoch: 40 [50/54 (93%)]\tTrain Loss: 0.000601\n",
            "\n",
            "Train set: Average loss: 0.0009, Accuracy: 425/425 (100%)\n",
            "\n",
            "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            "score [6.22764128e-05 1.34065850e-02 1.44330580e-02 2.37659272e-02\n",
            " 1.07181193e-04 8.71263212e-04 2.42960900e-01 1.94396675e-02\n",
            " 2.19828293e-01 9.34798941e-02 8.30461860e-01 8.03232118e-02\n",
            " 3.22751731e-01 4.54796739e-02 7.31717825e-01 1.05384133e-05\n",
            " 7.39808866e-06 1.38884827e-01 3.02218623e-03 5.18676650e-04\n",
            " 2.53735930e-02 9.62532759e-01 1.72674417e-01 9.99959588e-01\n",
            " 9.65648830e-01 9.92022753e-01 9.99994159e-01 2.44550314e-02\n",
            " 1.45025307e-03 7.82921258e-03 2.29684517e-01 1.34533435e-01\n",
            " 4.87086833e-01 4.72843709e-11 4.87972249e-12 6.17061276e-04\n",
            " 2.15650245e-04 3.61484110e-01 2.35992476e-01 6.50532031e-03\n",
            " 2.04924285e-01 5.36516488e-01 1.35923363e-02 1.93659756e-02\n",
            " 9.11934078e-01 3.34002003e-02 7.50677645e-01 2.45899037e-01\n",
            " 7.60360420e-01 9.90891278e-01 9.77659404e-01 2.97883523e-10\n",
            " 1.09574883e-08 1.76100426e-10 4.68983036e-03 2.31061549e-07\n",
            " 1.51164472e-01 9.17080467e-10 5.73136720e-12 8.07782821e-03\n",
            " 9.99991894e-01 9.99752820e-01 9.99648213e-01 9.99934554e-01\n",
            " 9.94299531e-01 9.99966502e-01 9.33451235e-01 9.99999523e-01\n",
            " 1.00000000e+00 9.99957800e-01 7.78468549e-01 3.68970007e-01\n",
            " 9.96382594e-01 9.74661887e-01 9.93499279e-01 9.90041852e-01\n",
            " 9.99911904e-01 9.99969363e-01 9.99955773e-01 9.99649167e-01\n",
            " 9.99998689e-01 1.00000000e+00 9.99999642e-01 9.82318521e-01\n",
            " 6.82504177e-01 9.99996185e-01 9.99910474e-01 9.99854922e-01\n",
            " 1.49020582e-01 1.00000000e+00 9.99979258e-01 9.96841311e-01\n",
            " 9.04904492e-03 9.99999762e-01 1.00000000e+00 1.00000000e+00\n",
            " 1.95405573e-01 9.90786314e-01 9.59020853e-01 9.81229901e-01\n",
            " 3.33314866e-01 9.81304407e-01 9.92500603e-01 3.69034968e-02\n",
            " 2.33728644e-02 2.38993287e-01 9.99991536e-01 9.79412675e-01\n",
            " 8.65810990e-01 7.86943734e-01 9.96095240e-01 9.55138743e-01\n",
            " 9.99672294e-01 9.98400271e-01 8.55605185e-01 9.46391523e-01\n",
            " 1.00000000e+00 9.99999881e-01]\n",
            "predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 1.\n",
            " 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 1. 0.\n",
            " 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 0. 1. 1. 1.\n",
            " 0. 1. 1. 1. 0. 1. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            "vote_pred [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1.\n",
            " 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1.\n",
            " 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 0. 1. 1. 0. 0. 1. 1. 1.\n",
            " 0. 1. 1. 1. 0. 1. 1. 0. 0. 0. 1. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1.]\n",
            "targetlist [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            "TP= 44 TN= 48 FN= 14 FP= 12\n",
            "TP+FP 56\n",
            "precision 0.7857142857142857\n",
            "recall 0.7586206896551724\n",
            "F1 0.7719298245614034\n",
            "acc 0.7796610169491526\n",
            "AUCp 0.7793103448275863\n",
            "AUC 0.8824712643678161\n",
            "\n",
            " The epoch is 40, average recall: 0.7586, average precision: 0.7857,average F1: 0.7719, average accuracy: 0.7797, average AUC: 0.8825\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0hLEqwPTQ-D2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0b3e2386-8788-479e-ddd3-5bb312217c75"
      },
      "source": [
        "\n",
        "# test\n",
        "bs = 10\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "r_list = []\n",
        "p_list = []\n",
        "acc_list = []\n",
        "AUC_list = []\n",
        "# TP = 0\n",
        "# TN = 0\n",
        "# FN = 0\n",
        "# FP = 0\n",
        "vote_pred = np.zeros(testset.__len__())\n",
        "vote_score = np.zeros(testset.__len__())\n",
        "\n",
        "#optimizer = optim.SGD(model.parameters(), lr=0.001, momentum = 0.9)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
        "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=10)\n",
        "scheduler = StepLR(optimizer, step_size=1)\n",
        "\n",
        "total_epoch = 10\n",
        "for epoch in range(1, total_epoch+1):\n",
        "    \n",
        "    targetlist, scorelist, predlist = test(epoch)\n",
        "#     print('target',targetlist)\n",
        "#     print('score',scorelist)\n",
        "#     print('predict',predlist)\n",
        "    vote_pred = vote_pred + predlist \n",
        "    vote_score = vote_score + scorelist \n",
        "    \n",
        "    TP = ((predlist == 1) & (targetlist == 1)).sum()\n",
        "    TN = ((predlist == 0) & (targetlist == 0)).sum()\n",
        "    FN = ((predlist == 0) & (targetlist == 1)).sum()\n",
        "    FP = ((predlist == 1) & (targetlist == 0)).sum()\n",
        "\n",
        "    print('TP=',TP,'TN=',TN,'FN=',FN,'FP=',FP)\n",
        "    print('TP+FP',TP+FP)\n",
        "    p = TP / (TP + FP)\n",
        "    print('precision',p)\n",
        "    p = TP / (TP + FP)\n",
        "    r = TP / (TP + FN)\n",
        "    print('recall',r)\n",
        "    F1 = 2 * r * p / (r + p)\n",
        "    acc = (TP + TN) / (TP + TN + FP + FN)\n",
        "    print('F1',F1)\n",
        "    print('acc',acc)\n",
        "    AUC = roc_auc_score(targetlist, vote_score)\n",
        "    print('AUC', AUC)\n",
        "\n",
        "    if epoch % votenum == 0:\n",
        "        \n",
        "        # major vote\n",
        "        vote_pred[vote_pred <= (votenum/2)] = 0\n",
        "        vote_pred[vote_pred > (votenum/2)] = 1\n",
        "        \n",
        "#         print('vote_pred', vote_pred)\n",
        "#         print('targetlist', targetlist)\n",
        "        TP = ((vote_pred == 1) & (targetlist == 1)).sum()\n",
        "        TN = ((vote_pred == 0) & (targetlist == 0)).sum()\n",
        "        FN = ((vote_pred == 0) & (targetlist == 1)).sum()\n",
        "        FP = ((vote_pred == 1) & (targetlist == 0)).sum()\n",
        "        \n",
        "        print('TP=',TP,'TN=',TN,'FN=',FN,'FP=',FP)\n",
        "        print('TP+FP',TP+FP)\n",
        "        p = TP / (TP + FP)\n",
        "        print('precision',p)\n",
        "        p = TP / (TP + FP)\n",
        "        r = TP / (TP + FN)\n",
        "        print('recall',r)\n",
        "        F1 = 2 * r * p / (r + p)\n",
        "        acc = (TP + TN) / (TP + TN + FP + FN)\n",
        "        print('F1',F1)\n",
        "        print('acc',acc)\n",
        "        AUC = roc_auc_score(targetlist, vote_score)\n",
        "        print('AUC', AUC)\n",
        "        \n",
        "        \n",
        "#         f = open('model_result/{modelname}.txt', 'a+')\n",
        "#         f.write('precision, recall, F1, acc= \\n')\n",
        "#         f.writelines(str(p))\n",
        "#         f.writelines('\\n')\n",
        "#         f.writelines(str(r))\n",
        "#         f.writelines('\\n')\n",
        "#         f.writelines(str(F1))\n",
        "#         f.writelines('\\n')\n",
        "#         f.writelines(str(acc))\n",
        "#         f.writelines('\\n')\n",
        "#         f.close()\n",
        "        \n",
        "        \n",
        "        vote_pred = np.zeros((1,testset.__len__()))\n",
        "        vote_score = np.zeros(testset.__len__())\n",
        "        print('vote_pred',vote_pred)\n",
        "        print('\\n The epoch is {}, average recall: {:.4f}, average precision: {:.4f},average F1: {:.4f}, average accuracy: {:.4f}, average AUC: {:.4f}'.format(\n",
        "        epoch, r, p, F1, acc, AUC))\n",
        "\n",
        "        f = open(f'modelb7_result/test_{modelname}.txt', 'a+')\n",
        "        f.write('\\n The epoch is {}, average recall: {:.4f}, average precision: {:.4f},average F1: {:.4f}, average accuracy: {:.4f}, average AUC: {:.4f}'.format(\n",
        "        epoch, r, p, F1, acc, AUC))\n",
        "        f.close()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TP= 90 TN= 76 FN= 15 FP= 22\n",
            "TP+FP 112\n",
            "precision 0.8035714285714286\n",
            "recall 0.8571428571428571\n",
            "F1 0.8294930875576038\n",
            "acc 0.8177339901477833\n",
            "AUC 0.8761904761904762\n",
            "TP= 90 TN= 76 FN= 15 FP= 22\n",
            "TP+FP 112\n",
            "precision 0.8035714285714286\n",
            "recall 0.8571428571428571\n",
            "F1 0.8294930875576038\n",
            "acc 0.8177339901477833\n",
            "AUC 0.8761904761904762\n",
            "TP= 90 TN= 76 FN= 15 FP= 22\n",
            "TP+FP 112\n",
            "precision 0.8035714285714286\n",
            "recall 0.8571428571428571\n",
            "F1 0.8294930875576038\n",
            "acc 0.8177339901477833\n",
            "AUC 0.8761904761904762\n",
            "TP= 90 TN= 76 FN= 15 FP= 22\n",
            "TP+FP 112\n",
            "precision 0.8035714285714286\n",
            "recall 0.8571428571428571\n",
            "F1 0.8294930875576038\n",
            "acc 0.8177339901477833\n",
            "AUC 0.8761904761904762\n",
            "TP= 90 TN= 76 FN= 15 FP= 22\n",
            "TP+FP 112\n",
            "precision 0.8035714285714286\n",
            "recall 0.8571428571428571\n",
            "F1 0.8294930875576038\n",
            "acc 0.8177339901477833\n",
            "AUC 0.8761904761904762\n",
            "TP= 90 TN= 76 FN= 15 FP= 22\n",
            "TP+FP 112\n",
            "precision 0.8035714285714286\n",
            "recall 0.8571428571428571\n",
            "F1 0.8294930875576038\n",
            "acc 0.8177339901477833\n",
            "AUC 0.8761904761904762\n",
            "TP= 90 TN= 76 FN= 15 FP= 22\n",
            "TP+FP 112\n",
            "precision 0.8035714285714286\n",
            "recall 0.8571428571428571\n",
            "F1 0.8294930875576038\n",
            "acc 0.8177339901477833\n",
            "AUC 0.8761904761904762\n",
            "TP= 90 TN= 76 FN= 15 FP= 22\n",
            "TP+FP 112\n",
            "precision 0.8035714285714286\n",
            "recall 0.8571428571428571\n",
            "F1 0.8294930875576038\n",
            "acc 0.8177339901477833\n",
            "AUC 0.8761904761904762\n",
            "TP= 90 TN= 76 FN= 15 FP= 22\n",
            "TP+FP 112\n",
            "precision 0.8035714285714286\n",
            "recall 0.8571428571428571\n",
            "F1 0.8294930875576038\n",
            "acc 0.8177339901477833\n",
            "AUC 0.8761904761904762\n",
            "TP= 90 TN= 76 FN= 15 FP= 22\n",
            "TP+FP 112\n",
            "precision 0.8035714285714286\n",
            "recall 0.8571428571428571\n",
            "F1 0.8294930875576038\n",
            "acc 0.8177339901477833\n",
            "AUC 0.8761904761904762\n",
            "TP= 90 TN= 76 FN= 15 FP= 22\n",
            "TP+FP 112\n",
            "precision 0.8035714285714286\n",
            "recall 0.8571428571428571\n",
            "F1 0.8294930875576038\n",
            "acc 0.8177339901477833\n",
            "AUC 0.8761904761904762\n",
            "vote_pred [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
            "\n",
            " The epoch is 10, average recall: 0.8571, average precision: 0.8036,average F1: 0.8295, average accuracy: 0.8177, average AUC: 0.8762\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DTcrntk1Q_1l"
      },
      "source": [
        "### Step 05: Save result and file pth"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VzoBL5WIYX5X",
        "outputId": "f2620557-fe45-47e8-d7a3-598bdd0e3926"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xgvuREUVYS_Y"
      },
      "source": [
        "!cp '/content/train_10epochs_model_backup/efficientNet-b7.pth' '/content/drive/MyDrive/REPORT/Research/EfficientNet/model_efficientNet-b7.pth'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nHbngMFEZ_BA"
      },
      "source": [
        "%cp '/content/modelb7_result/efficientNet-b7.txt' '/content/drive/MyDrive/REPORT/Research/EfficientNet/result_b7.txt'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3vw_vxAjYUHu"
      },
      "source": [
        "### Predicted "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1dAMB5OZYTd8"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}